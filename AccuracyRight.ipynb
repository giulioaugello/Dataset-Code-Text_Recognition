{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "from numpy import arange, mean, std\n",
    "from sklearn import preprocessing\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "         AxMean     AxMed     AxStd     AxVar     AxMax     AxMin    AyMean  \\\n0     -0.955980 -1.098000  0.909562  0.827302  0.271050 -2.202000  2.796702   \n1      0.582750  0.504000  0.497610  0.247616  1.228950 -0.115950  1.239209   \n2     -1.665956 -1.339950  0.780396  0.609017 -0.553950 -2.503950  2.015613   \n3     -0.068662 -0.181500  0.477223  0.227742  0.640050 -0.667050  1.617863   \n4      0.099825  0.120000  0.297081  0.088257  0.504000 -0.463050  1.194500   \n...         ...       ...       ...       ...       ...       ...       ...   \n25620 -1.753162 -1.707077  0.092171  0.008495 -1.707077 -1.891418  1.075600   \n25621  1.156399  1.051056  0.210686  0.044388  1.472427  1.051056  0.071224   \n25622 -0.220276 -0.220276  0.000000  0.000000 -0.220276 -0.220276  0.876282   \n25623 -0.143650 -0.095764  0.095772  0.009172 -0.095764 -0.287308  0.764942   \n25624 -0.229252 -0.244217  0.029930  0.000896 -0.184357 -0.244217  1.009148   \n\n          AyMed     AyStd     AyVar  ...  PreMean  PreMed  PreStd  PreVar  \\\n0      2.635050  0.343638  0.118087  ...      1.0     1.0     0.0     0.0   \n1      1.243950  0.195806  0.038340  ...      1.0     1.0     0.0     0.0   \n2      2.050950  0.320559  0.102758  ...      1.0     1.0     0.0     0.0   \n3      1.508550  0.337742  0.114069  ...      1.0     1.0     0.0     0.0   \n4      1.156500  0.209884  0.044051  ...      1.0     1.0     0.0     0.0   \n...         ...       ...       ...  ...      ...     ...     ...     ...   \n25620  1.108521  0.065842  0.004335  ...      1.0     1.0     0.0     0.0   \n25621  0.093369  0.044289  0.001961  ...      1.0     1.0     0.0     0.0   \n25622  0.876282  0.000000  0.000000  ...      1.0     1.0     0.0     0.0   \n25623  0.735016  0.059853  0.003582  ...      1.0     1.0     0.0     0.0   \n25624  1.060623  0.102951  0.010599  ...      1.0     1.0     0.0     0.0   \n\n       PreMax  PreMin  View    User   Hand      Smartphone  \n0         1.0     1.0     w  Biagio  RIGHT        REALME 7  \n1         1.0     1.0     h  Biagio  RIGHT        REALME 7  \n2         1.0     1.0     a  Biagio  RIGHT        REALME 7  \n3         1.0     1.0     t  Biagio  RIGHT        REALME 7  \n4         1.0     1.0        Biagio  RIGHT        REALME 7  \n...       ...     ...   ...     ...    ...             ...  \n25620     1.0     1.0     a  Giulio  RIGHT  ASUS ZENFONE 3  \n25621     1.0     1.0     l  Giulio  RIGHT  ASUS ZENFONE 3  \n25622     1.0     1.0     i  Giulio  RIGHT  ASUS ZENFONE 3  \n25623     1.0     1.0     t  Giulio  RIGHT  ASUS ZENFONE 3  \n25624     1.0     1.0     y  Giulio  RIGHT  ASUS ZENFONE 3  \n\n[25625 rows x 280 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>AxMean</th>\n      <th>AxMed</th>\n      <th>AxStd</th>\n      <th>AxVar</th>\n      <th>AxMax</th>\n      <th>AxMin</th>\n      <th>AyMean</th>\n      <th>AyMed</th>\n      <th>AyStd</th>\n      <th>AyVar</th>\n      <th>...</th>\n      <th>PreMean</th>\n      <th>PreMed</th>\n      <th>PreStd</th>\n      <th>PreVar</th>\n      <th>PreMax</th>\n      <th>PreMin</th>\n      <th>View</th>\n      <th>User</th>\n      <th>Hand</th>\n      <th>Smartphone</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.955980</td>\n      <td>-1.098000</td>\n      <td>0.909562</td>\n      <td>0.827302</td>\n      <td>0.271050</td>\n      <td>-2.202000</td>\n      <td>2.796702</td>\n      <td>2.635050</td>\n      <td>0.343638</td>\n      <td>0.118087</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>w</td>\n      <td>Biagio</td>\n      <td>RIGHT</td>\n      <td>REALME 7</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.582750</td>\n      <td>0.504000</td>\n      <td>0.497610</td>\n      <td>0.247616</td>\n      <td>1.228950</td>\n      <td>-0.115950</td>\n      <td>1.239209</td>\n      <td>1.243950</td>\n      <td>0.195806</td>\n      <td>0.038340</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>h</td>\n      <td>Biagio</td>\n      <td>RIGHT</td>\n      <td>REALME 7</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-1.665956</td>\n      <td>-1.339950</td>\n      <td>0.780396</td>\n      <td>0.609017</td>\n      <td>-0.553950</td>\n      <td>-2.503950</td>\n      <td>2.015613</td>\n      <td>2.050950</td>\n      <td>0.320559</td>\n      <td>0.102758</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>a</td>\n      <td>Biagio</td>\n      <td>RIGHT</td>\n      <td>REALME 7</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.068662</td>\n      <td>-0.181500</td>\n      <td>0.477223</td>\n      <td>0.227742</td>\n      <td>0.640050</td>\n      <td>-0.667050</td>\n      <td>1.617863</td>\n      <td>1.508550</td>\n      <td>0.337742</td>\n      <td>0.114069</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>t</td>\n      <td>Biagio</td>\n      <td>RIGHT</td>\n      <td>REALME 7</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.099825</td>\n      <td>0.120000</td>\n      <td>0.297081</td>\n      <td>0.088257</td>\n      <td>0.504000</td>\n      <td>-0.463050</td>\n      <td>1.194500</td>\n      <td>1.156500</td>\n      <td>0.209884</td>\n      <td>0.044051</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td></td>\n      <td>Biagio</td>\n      <td>RIGHT</td>\n      <td>REALME 7</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>25620</th>\n      <td>-1.753162</td>\n      <td>-1.707077</td>\n      <td>0.092171</td>\n      <td>0.008495</td>\n      <td>-1.707077</td>\n      <td>-1.891418</td>\n      <td>1.075600</td>\n      <td>1.108521</td>\n      <td>0.065842</td>\n      <td>0.004335</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>a</td>\n      <td>Giulio</td>\n      <td>RIGHT</td>\n      <td>ASUS ZENFONE 3</td>\n    </tr>\n    <tr>\n      <th>25621</th>\n      <td>1.156399</td>\n      <td>1.051056</td>\n      <td>0.210686</td>\n      <td>0.044388</td>\n      <td>1.472427</td>\n      <td>1.051056</td>\n      <td>0.071224</td>\n      <td>0.093369</td>\n      <td>0.044289</td>\n      <td>0.001961</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>l</td>\n      <td>Giulio</td>\n      <td>RIGHT</td>\n      <td>ASUS ZENFONE 3</td>\n    </tr>\n    <tr>\n      <th>25622</th>\n      <td>-0.220276</td>\n      <td>-0.220276</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>-0.220276</td>\n      <td>-0.220276</td>\n      <td>0.876282</td>\n      <td>0.876282</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>i</td>\n      <td>Giulio</td>\n      <td>RIGHT</td>\n      <td>ASUS ZENFONE 3</td>\n    </tr>\n    <tr>\n      <th>25623</th>\n      <td>-0.143650</td>\n      <td>-0.095764</td>\n      <td>0.095772</td>\n      <td>0.009172</td>\n      <td>-0.095764</td>\n      <td>-0.287308</td>\n      <td>0.764942</td>\n      <td>0.735016</td>\n      <td>0.059853</td>\n      <td>0.003582</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>t</td>\n      <td>Giulio</td>\n      <td>RIGHT</td>\n      <td>ASUS ZENFONE 3</td>\n    </tr>\n    <tr>\n      <th>25624</th>\n      <td>-0.229252</td>\n      <td>-0.244217</td>\n      <td>0.029930</td>\n      <td>0.000896</td>\n      <td>-0.184357</td>\n      <td>-0.244217</td>\n      <td>1.009148</td>\n      <td>1.060623</td>\n      <td>0.102951</td>\n      <td>0.010599</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>y</td>\n      <td>Giulio</td>\n      <td>RIGHT</td>\n      <td>ASUS ZENFONE 3</td>\n    </tr>\n  </tbody>\n</table>\n<p>25625 rows Ã— 280 columns</p>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import csv\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\giuli\\\\Desktop\\\\new ds\\\\merged\\\\mergedFile.csv\")\n",
    "df\n",
    "# print(df.describe())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Normalization between -1 and 1\n",
    "df_norm = df.copy()\n",
    "\n",
    "notnorm = ['View','User', 'Hand', 'Smartphone']\n",
    "min_max_scaler = preprocessing.MinMaxScaler(feature_range=(-1,1))\n",
    "\n",
    "for col in list(df_norm.columns):\n",
    "    if col in notnorm:\n",
    "        continue\n",
    "    x = df_norm[[col]].values.astype(float) # cast to float\n",
    "    x_scaled = min_max_scaler.fit_transform(x) # fit data and transform it\n",
    "    df_norm[[col]] = x_scaled"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "         AxMean     AxMed     AxStd     AxVar     AxMax     AxMin    AyMean  \\\n0      0.035564  0.050492 -0.246425 -0.716062  0.127137 -0.053982 -0.030285   \n1      0.418963  0.425894 -0.587728 -0.915016  0.347631  0.426174 -0.401803   \n2     -0.141338 -0.006206 -0.353439 -0.790980 -0.062766 -0.123483 -0.216603   \n3      0.256653  0.265258 -0.604619 -0.921837  0.212075  0.299325 -0.311481   \n4      0.298634  0.335910 -0.753867 -0.969709  0.180758  0.346281 -0.412468   \n...         ...       ...       ...       ...       ...       ...       ...   \n25620 -0.163067 -0.092236 -0.923636 -0.997084 -0.328200  0.017506 -0.440830   \n25621  0.561896  0.554088 -0.825446 -0.984765  0.403676  0.694790 -0.680409   \n25622  0.218876  0.256172 -1.000000 -1.000000  0.014041  0.402161 -0.488374   \n25623  0.237969  0.285349 -0.920653 -0.996852  0.042701  0.386732 -0.514933   \n25624  0.216640  0.250562 -0.975203 -0.999693  0.022309  0.396651 -0.456681   \n\n          AyMed     AyStd     AyVar  ...  PreMean  PreMed  PreStd  PreVar  \\\n0     -0.040270 -0.422755 -0.833394  ...      1.0     1.0    -1.0    -1.0   \n1     -0.365685 -0.671085 -0.945907  ...      1.0     1.0    -1.0    -1.0   \n2     -0.176906 -0.461523 -0.855021  ...      1.0     1.0    -1.0    -1.0   \n3     -0.303788 -0.432660 -0.839063  ...      1.0     1.0    -1.0    -1.0   \n4     -0.386142 -0.647436 -0.937849  ...      1.0     1.0    -1.0    -1.0   \n...         ...       ...       ...  ...      ...     ...     ...     ...   \n25620 -0.397365 -0.889399 -0.993884  ...      1.0     1.0    -1.0    -1.0   \n25621 -0.634836 -0.925604 -0.997233  ...      1.0     1.0    -1.0    -1.0   \n25622 -0.451692 -1.000000 -1.000000  ...      1.0     1.0    -1.0    -1.0   \n25623 -0.484738 -0.899459 -0.994946  ...      1.0     1.0    -1.0    -1.0   \n25624 -0.408570 -0.827062 -0.985046  ...      1.0     1.0    -1.0    -1.0   \n\n       PreMax  PreMin  View    User   Hand      Smartphone  \n0         1.0     1.0     w  Biagio  RIGHT        REALME 7  \n1         1.0     1.0     h  Biagio  RIGHT        REALME 7  \n2         1.0     1.0     a  Biagio  RIGHT        REALME 7  \n3         1.0     1.0     t  Biagio  RIGHT        REALME 7  \n4         1.0     1.0        Biagio  RIGHT        REALME 7  \n...       ...     ...   ...     ...    ...             ...  \n25620     1.0     1.0     a  Giulio  RIGHT  ASUS ZENFONE 3  \n25621     1.0     1.0     l  Giulio  RIGHT  ASUS ZENFONE 3  \n25622     1.0     1.0     i  Giulio  RIGHT  ASUS ZENFONE 3  \n25623     1.0     1.0     t  Giulio  RIGHT  ASUS ZENFONE 3  \n25624     1.0     1.0     y  Giulio  RIGHT  ASUS ZENFONE 3  \n\n[25625 rows x 280 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>AxMean</th>\n      <th>AxMed</th>\n      <th>AxStd</th>\n      <th>AxVar</th>\n      <th>AxMax</th>\n      <th>AxMin</th>\n      <th>AyMean</th>\n      <th>AyMed</th>\n      <th>AyStd</th>\n      <th>AyVar</th>\n      <th>...</th>\n      <th>PreMean</th>\n      <th>PreMed</th>\n      <th>PreStd</th>\n      <th>PreVar</th>\n      <th>PreMax</th>\n      <th>PreMin</th>\n      <th>View</th>\n      <th>User</th>\n      <th>Hand</th>\n      <th>Smartphone</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.035564</td>\n      <td>0.050492</td>\n      <td>-0.246425</td>\n      <td>-0.716062</td>\n      <td>0.127137</td>\n      <td>-0.053982</td>\n      <td>-0.030285</td>\n      <td>-0.040270</td>\n      <td>-0.422755</td>\n      <td>-0.833394</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>w</td>\n      <td>Biagio</td>\n      <td>RIGHT</td>\n      <td>REALME 7</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.418963</td>\n      <td>0.425894</td>\n      <td>-0.587728</td>\n      <td>-0.915016</td>\n      <td>0.347631</td>\n      <td>0.426174</td>\n      <td>-0.401803</td>\n      <td>-0.365685</td>\n      <td>-0.671085</td>\n      <td>-0.945907</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>h</td>\n      <td>Biagio</td>\n      <td>RIGHT</td>\n      <td>REALME 7</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.141338</td>\n      <td>-0.006206</td>\n      <td>-0.353439</td>\n      <td>-0.790980</td>\n      <td>-0.062766</td>\n      <td>-0.123483</td>\n      <td>-0.216603</td>\n      <td>-0.176906</td>\n      <td>-0.461523</td>\n      <td>-0.855021</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>a</td>\n      <td>Biagio</td>\n      <td>RIGHT</td>\n      <td>REALME 7</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.256653</td>\n      <td>0.265258</td>\n      <td>-0.604619</td>\n      <td>-0.921837</td>\n      <td>0.212075</td>\n      <td>0.299325</td>\n      <td>-0.311481</td>\n      <td>-0.303788</td>\n      <td>-0.432660</td>\n      <td>-0.839063</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>t</td>\n      <td>Biagio</td>\n      <td>RIGHT</td>\n      <td>REALME 7</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.298634</td>\n      <td>0.335910</td>\n      <td>-0.753867</td>\n      <td>-0.969709</td>\n      <td>0.180758</td>\n      <td>0.346281</td>\n      <td>-0.412468</td>\n      <td>-0.386142</td>\n      <td>-0.647436</td>\n      <td>-0.937849</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td></td>\n      <td>Biagio</td>\n      <td>RIGHT</td>\n      <td>REALME 7</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>25620</th>\n      <td>-0.163067</td>\n      <td>-0.092236</td>\n      <td>-0.923636</td>\n      <td>-0.997084</td>\n      <td>-0.328200</td>\n      <td>0.017506</td>\n      <td>-0.440830</td>\n      <td>-0.397365</td>\n      <td>-0.889399</td>\n      <td>-0.993884</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>a</td>\n      <td>Giulio</td>\n      <td>RIGHT</td>\n      <td>ASUS ZENFONE 3</td>\n    </tr>\n    <tr>\n      <th>25621</th>\n      <td>0.561896</td>\n      <td>0.554088</td>\n      <td>-0.825446</td>\n      <td>-0.984765</td>\n      <td>0.403676</td>\n      <td>0.694790</td>\n      <td>-0.680409</td>\n      <td>-0.634836</td>\n      <td>-0.925604</td>\n      <td>-0.997233</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>l</td>\n      <td>Giulio</td>\n      <td>RIGHT</td>\n      <td>ASUS ZENFONE 3</td>\n    </tr>\n    <tr>\n      <th>25622</th>\n      <td>0.218876</td>\n      <td>0.256172</td>\n      <td>-1.000000</td>\n      <td>-1.000000</td>\n      <td>0.014041</td>\n      <td>0.402161</td>\n      <td>-0.488374</td>\n      <td>-0.451692</td>\n      <td>-1.000000</td>\n      <td>-1.000000</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>i</td>\n      <td>Giulio</td>\n      <td>RIGHT</td>\n      <td>ASUS ZENFONE 3</td>\n    </tr>\n    <tr>\n      <th>25623</th>\n      <td>0.237969</td>\n      <td>0.285349</td>\n      <td>-0.920653</td>\n      <td>-0.996852</td>\n      <td>0.042701</td>\n      <td>0.386732</td>\n      <td>-0.514933</td>\n      <td>-0.484738</td>\n      <td>-0.899459</td>\n      <td>-0.994946</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>t</td>\n      <td>Giulio</td>\n      <td>RIGHT</td>\n      <td>ASUS ZENFONE 3</td>\n    </tr>\n    <tr>\n      <th>25624</th>\n      <td>0.216640</td>\n      <td>0.250562</td>\n      <td>-0.975203</td>\n      <td>-0.999693</td>\n      <td>0.022309</td>\n      <td>0.396651</td>\n      <td>-0.456681</td>\n      <td>-0.408570</td>\n      <td>-0.827062</td>\n      <td>-0.985046</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>y</td>\n      <td>Giulio</td>\n      <td>RIGHT</td>\n      <td>ASUS ZENFONE 3</td>\n    </tr>\n  </tbody>\n</table>\n<p>25625 rows Ã— 280 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_norm.describe()\n",
    "df = df_norm\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "         AxMean     AxMed     AxStd     AxVar     AxMax     AxMin    AyMean  \\\n0      0.035564  0.050492 -0.246425 -0.716062  0.127137 -0.053982 -0.030285   \n1      0.418963  0.425894 -0.587728 -0.915016  0.347631  0.426174 -0.401803   \n2     -0.141338 -0.006206 -0.353439 -0.790980 -0.062766 -0.123483 -0.216603   \n3      0.256653  0.265258 -0.604619 -0.921837  0.212075  0.299325 -0.311481   \n4      0.298634  0.335910 -0.753867 -0.969709  0.180758  0.346281 -0.412468   \n...         ...       ...       ...       ...       ...       ...       ...   \n25620 -0.163067 -0.092236 -0.923636 -0.997084 -0.328200  0.017506 -0.440830   \n25621  0.561896  0.554088 -0.825446 -0.984765  0.403676  0.694790 -0.680409   \n25622  0.218876  0.256172 -1.000000 -1.000000  0.014041  0.402161 -0.488374   \n25623  0.237969  0.285349 -0.920653 -0.996852  0.042701  0.386732 -0.514933   \n25624  0.216640  0.250562 -0.975203 -0.999693  0.022309  0.396651 -0.456681   \n\n          AyMed     AyStd     AyVar  ...  PreVar  PreMax  PreMin  View  \\\n0     -0.040270 -0.422755 -0.833394  ...    -1.0     1.0     1.0     w   \n1     -0.365685 -0.671085 -0.945907  ...    -1.0     1.0     1.0     h   \n2     -0.176906 -0.461523 -0.855021  ...    -1.0     1.0     1.0     a   \n3     -0.303788 -0.432660 -0.839063  ...    -1.0     1.0     1.0     t   \n4     -0.386142 -0.647436 -0.937849  ...    -1.0     1.0     1.0         \n...         ...       ...       ...  ...     ...     ...     ...   ...   \n25620 -0.397365 -0.889399 -0.993884  ...    -1.0     1.0     1.0     a   \n25621 -0.634836 -0.925604 -0.997233  ...    -1.0     1.0     1.0     l   \n25622 -0.451692 -1.000000 -1.000000  ...    -1.0     1.0     1.0     i   \n25623 -0.484738 -0.899459 -0.994946  ...    -1.0     1.0     1.0     t   \n25624 -0.408570 -0.827062 -0.985046  ...    -1.0     1.0     1.0     y   \n\n         User   Hand      Smartphone    x  y      dist  \n0      Biagio  RIGHT        REALME 7  2.0  0  1.118034  \n1      Biagio  RIGHT        REALME 7  6.5  1  5.000000  \n2      Biagio  RIGHT        REALME 7  1.5  1  0.000000  \n3      Biagio  RIGHT        REALME 7  5.0  0  3.640055  \n4      Biagio  RIGHT        REALME 7  5.0  3  4.031129  \n...       ...    ...             ...  ... ..       ...  \n25620  Giulio  RIGHT  ASUS ZENFONE 3  1.5  1  0.000000  \n25621  Giulio  RIGHT  ASUS ZENFONE 3  9.5  1  8.000000  \n25622  Giulio  RIGHT  ASUS ZENFONE 3  8.0  0  6.576473  \n25623  Giulio  RIGHT  ASUS ZENFONE 3  5.0  0  3.640055  \n25624  Giulio  RIGHT  ASUS ZENFONE 3  6.0  0  4.609772  \n\n[25625 rows x 283 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>AxMean</th>\n      <th>AxMed</th>\n      <th>AxStd</th>\n      <th>AxVar</th>\n      <th>AxMax</th>\n      <th>AxMin</th>\n      <th>AyMean</th>\n      <th>AyMed</th>\n      <th>AyStd</th>\n      <th>AyVar</th>\n      <th>...</th>\n      <th>PreVar</th>\n      <th>PreMax</th>\n      <th>PreMin</th>\n      <th>View</th>\n      <th>User</th>\n      <th>Hand</th>\n      <th>Smartphone</th>\n      <th>x</th>\n      <th>y</th>\n      <th>dist</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.035564</td>\n      <td>0.050492</td>\n      <td>-0.246425</td>\n      <td>-0.716062</td>\n      <td>0.127137</td>\n      <td>-0.053982</td>\n      <td>-0.030285</td>\n      <td>-0.040270</td>\n      <td>-0.422755</td>\n      <td>-0.833394</td>\n      <td>...</td>\n      <td>-1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>w</td>\n      <td>Biagio</td>\n      <td>RIGHT</td>\n      <td>REALME 7</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>1.118034</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.418963</td>\n      <td>0.425894</td>\n      <td>-0.587728</td>\n      <td>-0.915016</td>\n      <td>0.347631</td>\n      <td>0.426174</td>\n      <td>-0.401803</td>\n      <td>-0.365685</td>\n      <td>-0.671085</td>\n      <td>-0.945907</td>\n      <td>...</td>\n      <td>-1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>h</td>\n      <td>Biagio</td>\n      <td>RIGHT</td>\n      <td>REALME 7</td>\n      <td>6.5</td>\n      <td>1</td>\n      <td>5.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.141338</td>\n      <td>-0.006206</td>\n      <td>-0.353439</td>\n      <td>-0.790980</td>\n      <td>-0.062766</td>\n      <td>-0.123483</td>\n      <td>-0.216603</td>\n      <td>-0.176906</td>\n      <td>-0.461523</td>\n      <td>-0.855021</td>\n      <td>...</td>\n      <td>-1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>a</td>\n      <td>Biagio</td>\n      <td>RIGHT</td>\n      <td>REALME 7</td>\n      <td>1.5</td>\n      <td>1</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.256653</td>\n      <td>0.265258</td>\n      <td>-0.604619</td>\n      <td>-0.921837</td>\n      <td>0.212075</td>\n      <td>0.299325</td>\n      <td>-0.311481</td>\n      <td>-0.303788</td>\n      <td>-0.432660</td>\n      <td>-0.839063</td>\n      <td>...</td>\n      <td>-1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>t</td>\n      <td>Biagio</td>\n      <td>RIGHT</td>\n      <td>REALME 7</td>\n      <td>5.0</td>\n      <td>0</td>\n      <td>3.640055</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.298634</td>\n      <td>0.335910</td>\n      <td>-0.753867</td>\n      <td>-0.969709</td>\n      <td>0.180758</td>\n      <td>0.346281</td>\n      <td>-0.412468</td>\n      <td>-0.386142</td>\n      <td>-0.647436</td>\n      <td>-0.937849</td>\n      <td>...</td>\n      <td>-1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td></td>\n      <td>Biagio</td>\n      <td>RIGHT</td>\n      <td>REALME 7</td>\n      <td>5.0</td>\n      <td>3</td>\n      <td>4.031129</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>25620</th>\n      <td>-0.163067</td>\n      <td>-0.092236</td>\n      <td>-0.923636</td>\n      <td>-0.997084</td>\n      <td>-0.328200</td>\n      <td>0.017506</td>\n      <td>-0.440830</td>\n      <td>-0.397365</td>\n      <td>-0.889399</td>\n      <td>-0.993884</td>\n      <td>...</td>\n      <td>-1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>a</td>\n      <td>Giulio</td>\n      <td>RIGHT</td>\n      <td>ASUS ZENFONE 3</td>\n      <td>1.5</td>\n      <td>1</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25621</th>\n      <td>0.561896</td>\n      <td>0.554088</td>\n      <td>-0.825446</td>\n      <td>-0.984765</td>\n      <td>0.403676</td>\n      <td>0.694790</td>\n      <td>-0.680409</td>\n      <td>-0.634836</td>\n      <td>-0.925604</td>\n      <td>-0.997233</td>\n      <td>...</td>\n      <td>-1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>l</td>\n      <td>Giulio</td>\n      <td>RIGHT</td>\n      <td>ASUS ZENFONE 3</td>\n      <td>9.5</td>\n      <td>1</td>\n      <td>8.000000</td>\n    </tr>\n    <tr>\n      <th>25622</th>\n      <td>0.218876</td>\n      <td>0.256172</td>\n      <td>-1.000000</td>\n      <td>-1.000000</td>\n      <td>0.014041</td>\n      <td>0.402161</td>\n      <td>-0.488374</td>\n      <td>-0.451692</td>\n      <td>-1.000000</td>\n      <td>-1.000000</td>\n      <td>...</td>\n      <td>-1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>i</td>\n      <td>Giulio</td>\n      <td>RIGHT</td>\n      <td>ASUS ZENFONE 3</td>\n      <td>8.0</td>\n      <td>0</td>\n      <td>6.576473</td>\n    </tr>\n    <tr>\n      <th>25623</th>\n      <td>0.237969</td>\n      <td>0.285349</td>\n      <td>-0.920653</td>\n      <td>-0.996852</td>\n      <td>0.042701</td>\n      <td>0.386732</td>\n      <td>-0.514933</td>\n      <td>-0.484738</td>\n      <td>-0.899459</td>\n      <td>-0.994946</td>\n      <td>...</td>\n      <td>-1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>t</td>\n      <td>Giulio</td>\n      <td>RIGHT</td>\n      <td>ASUS ZENFONE 3</td>\n      <td>5.0</td>\n      <td>0</td>\n      <td>3.640055</td>\n    </tr>\n    <tr>\n      <th>25624</th>\n      <td>0.216640</td>\n      <td>0.250562</td>\n      <td>-0.975203</td>\n      <td>-0.999693</td>\n      <td>0.022309</td>\n      <td>0.396651</td>\n      <td>-0.456681</td>\n      <td>-0.408570</td>\n      <td>-0.827062</td>\n      <td>-0.985046</td>\n      <td>...</td>\n      <td>-1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>y</td>\n      <td>Giulio</td>\n      <td>RIGHT</td>\n      <td>ASUS ZENFONE 3</td>\n      <td>6.0</td>\n      <td>0</td>\n      <td>4.609772</td>\n    </tr>\n  </tbody>\n</table>\n<p>25625 rows Ã— 283 columns</p>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add letters\n",
    "letters = [\"a\",\"b\",\"c\",\"d\",\"e\",\"f\",\"g\",\"h\",\"i\",\"j\",\"k\",\"l\",\"m\",\"n\",\"o\",\"p\",\"q\",\"r\",\"s\",\"t\",\"u\",\"v\",\"w\",\"x\",\"y\",\"z\",\" \"]\n",
    "# add distances from left margin (xAxis) and top margin (yAxis) (start from 1)\n",
    "yAxis = [ 1,  2,  2,  1,  0,  1,  1,  1,  0,  1,  1,  1,  2,  2,  0,  0,  0,  0,  1,  0,  0,  2,  0,  2,  0,  2, 3]\n",
    "xAxis =[ 1.5,  6,  4,  3.5,  3,  4.5,  5.5, 6.5,  8,  7.5, 8.5,  9.5,  8,  7,  9,  10,  1,  4,  2.5,  5,  7,  5,  2,  3,  6,  2, 5]\n",
    "\n",
    "# pair of distances for letters\n",
    "dict_letters = {}\n",
    "for i, let in enumerate(letters):\n",
    "    dict_letters[let] = (xAxis[i], yAxis[i])\n",
    "\n",
    "center = \"a\"\n",
    "dict_dist = {}\n",
    "for let in letters:\n",
    "    dict_dist[let] = math.sqrt((dict_letters[center][0] - dict_letters[let][0])**2 + (dict_letters[center][1] - dict_letters[let][1])**2)\n",
    "\n",
    "# write in df distances and dist from center\n",
    "df.loc[:,'x'] = df.apply(lambda x: dict_letters[x['View']][0], axis=1)\n",
    "df.loc[:,'y'] = df.apply(lambda x: dict_letters[x['View']][1], axis=1)\n",
    "df.loc[:,'dist'] = df.apply(lambda x: dict_dist[x['View']], axis=1)\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "   View      dist     x  y    AxMean     AxMed     AxStd     AxVar     AxMax  \\\n0        4.031129   5.0  3  0.263204  0.296761 -0.746812 -0.953980  0.153360   \n1     a  0.000000   1.5  1 -0.158803 -0.102414 -0.562209 -0.852578 -0.165842   \n2     b  4.609772   6.0  2  0.259733  0.294619 -0.751112 -0.953407  0.142498   \n3     c  2.692582   4.0  2  0.176625  0.217678 -0.701008 -0.926940  0.089153   \n4     d  2.000000   3.5  1  0.066985  0.114844 -0.686925 -0.921824 -0.008257   \n5     e  1.802776   3.0  0 -0.023094  0.028453 -0.655597 -0.903264 -0.083387   \n6     f  3.000000   4.5  1  0.108786  0.152744 -0.727190 -0.938727  0.012964   \n7     g  4.000000   5.5  1  0.147901  0.191665 -0.720540 -0.940661  0.052033   \n8     h  5.000000   6.5  1  0.241819  0.277230 -0.748850 -0.951136  0.126884   \n9     i  6.576473   8.0  0  0.255855  0.290744 -0.740584 -0.949152  0.147189   \n10    j  6.000000   7.5  1  0.272473  0.309670 -0.755372 -0.956453  0.149772   \n11    k  7.000000   8.5  1  0.339304  0.369111 -0.727644 -0.945306  0.225544   \n12    l  8.000000   9.5  1  0.382233  0.405073 -0.638259 -0.906771  0.306763   \n13    m  6.576473   8.0  2  0.389207  0.418690 -0.707943 -0.937793  0.278708   \n14    n  5.590170   7.0  2  0.331634  0.362837 -0.743480 -0.951985  0.213202   \n15    o  7.566373   9.0  0  0.286624  0.319884 -0.733715 -0.947680  0.179723   \n16    p  8.558621  10.0  0  0.371898  0.392885 -0.601324 -0.892655  0.319657   \n17    q  1.118034   1.0  0 -0.264526 -0.198774 -0.531499 -0.839556 -0.256962   \n18    r  2.692582   4.0  0  0.069555  0.116442 -0.704776 -0.931194 -0.012913   \n19    s  1.000000   2.5  1 -0.006690  0.040577 -0.628584 -0.887011 -0.051007   \n20    t  3.640055   5.0  0  0.116050  0.163092 -0.724649 -0.940552  0.018509   \n21    u  5.590170   7.0  0  0.217092  0.254366 -0.722287 -0.941304  0.113238   \n22    v  3.640055   5.0  2  0.216043  0.255153 -0.725401 -0.945603  0.113690   \n23    w  1.118034   2.0  0 -0.123910 -0.070142 -0.526372 -0.828295 -0.125490   \n24    x  1.802776   3.0  2  0.118531  0.163655 -0.692257 -0.921865  0.037162   \n25    y  4.609772   6.0  0  0.171649  0.210967 -0.737031 -0.944728  0.064431   \n26    z  1.118034   2.0  2  0.023697  0.075727 -0.653561 -0.901157 -0.035312   \n\n       AxMin  ...  GrPitchStd  GrPitchVar  GrPitchMax  GrPitchMin   PreMean  \\\n0   0.343680  ...   -0.774955   -0.954829   -0.167063   -0.119677  0.344867   \n1  -0.113848  ...   -0.673249   -0.907875    0.282449    0.283081  0.374189   \n2   0.344963  ...   -0.827397   -0.969894   -0.172722   -0.112642  0.331312   \n3   0.239879  ...   -0.789811   -0.959105   -0.081256   -0.038746  0.361868   \n4   0.134961  ...   -0.754170   -0.944572    0.036505    0.063000  0.371255   \n5   0.042347  ...   -0.721175   -0.929613    0.131097    0.146652  0.342579   \n6   0.192386  ...   -0.774333   -0.950961   -0.018478    0.018350  0.351042   \n7   0.223479  ...   -0.791777   -0.958728   -0.042781    0.000280  0.374867   \n8   0.324900  ...   -0.816898   -0.968289   -0.146433   -0.090606  0.378174   \n9   0.335956  ...   -0.816679   -0.967333   -0.174606   -0.119239  0.362228   \n10  0.357694  ...   -0.833686   -0.973050   -0.186007   -0.125157  0.405134   \n11  0.408772  ...   -0.800661   -0.954912   -0.262451   -0.206437  0.376719   \n12  0.417367  ...   -0.745010   -0.936543   -0.322365   -0.275522  0.319872   \n13  0.442163  ...   -0.805700   -0.961176   -0.321530   -0.262477  0.388432   \n14  0.403625  ...   -0.804161   -0.960631   -0.251065   -0.193773  0.382777   \n15  0.362403  ...   -0.785088   -0.958472   -0.213353   -0.162598  0.351025   \n16  0.391443  ...   -0.697584   -0.923587   -0.308737   -0.274732  0.336231   \n17 -0.221133  ...   -0.645388   -0.888733    0.401593    0.395122  0.344465   \n18  0.141925  ...   -0.747409   -0.943475    0.035223    0.063408  0.352901   \n19  0.048894  ...   -0.738408   -0.937212    0.113424    0.131729  0.348505   \n20  0.190351  ...   -0.772339   -0.952973   -0.012277    0.025682  0.325021   \n21  0.293188  ...   -0.791973   -0.949892   -0.126995   -0.076215  0.311063   \n22  0.291929  ...   -0.823008   -0.971734   -0.123508   -0.068384  0.377762   \n23 -0.095749  ...   -0.659208   -0.906468    0.242984    0.242556  0.373852   \n24  0.184497  ...   -0.766687   -0.950972   -0.012482    0.022255  0.415190   \n25  0.256416  ...   -0.820402   -0.966527   -0.077335   -0.022922  0.332406   \n26  0.082856  ...   -0.751571   -0.944376    0.091066    0.115010  0.365227   \n\n      PreMed  PreStd  PreVar    PreMax    PreMin  \n0   0.344867    -1.0    -1.0  0.344867  0.344867  \n1   0.374189    -1.0    -1.0  0.374189  0.374189  \n2   0.331312    -1.0    -1.0  0.331312  0.331312  \n3   0.361868    -1.0    -1.0  0.361868  0.361868  \n4   0.371255    -1.0    -1.0  0.371255  0.371255  \n5   0.342579    -1.0    -1.0  0.342579  0.342579  \n6   0.351042    -1.0    -1.0  0.351042  0.351042  \n7   0.374867    -1.0    -1.0  0.374867  0.374867  \n8   0.378174    -1.0    -1.0  0.378174  0.378174  \n9   0.362228    -1.0    -1.0  0.362228  0.362228  \n10  0.405134    -1.0    -1.0  0.405134  0.405134  \n11  0.376719    -1.0    -1.0  0.376719  0.376719  \n12  0.319872    -1.0    -1.0  0.319872  0.319872  \n13  0.388432    -1.0    -1.0  0.388432  0.388432  \n14  0.382777    -1.0    -1.0  0.382777  0.382777  \n15  0.351025    -1.0    -1.0  0.351025  0.351025  \n16  0.336231    -1.0    -1.0  0.336231  0.336231  \n17  0.344465    -1.0    -1.0  0.344465  0.344465  \n18  0.352901    -1.0    -1.0  0.352901  0.352901  \n19  0.348505    -1.0    -1.0  0.348505  0.348505  \n20  0.325021    -1.0    -1.0  0.325021  0.325021  \n21  0.311063    -1.0    -1.0  0.311063  0.311063  \n22  0.377762    -1.0    -1.0  0.377762  0.377762  \n23  0.373852    -1.0    -1.0  0.373852  0.373852  \n24  0.415190    -1.0    -1.0  0.415190  0.415190  \n25  0.332406    -1.0    -1.0  0.332406  0.332406  \n26  0.365227    -1.0    -1.0  0.365227  0.365227  \n\n[27 rows x 280 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>View</th>\n      <th>dist</th>\n      <th>x</th>\n      <th>y</th>\n      <th>AxMean</th>\n      <th>AxMed</th>\n      <th>AxStd</th>\n      <th>AxVar</th>\n      <th>AxMax</th>\n      <th>AxMin</th>\n      <th>...</th>\n      <th>GrPitchStd</th>\n      <th>GrPitchVar</th>\n      <th>GrPitchMax</th>\n      <th>GrPitchMin</th>\n      <th>PreMean</th>\n      <th>PreMed</th>\n      <th>PreStd</th>\n      <th>PreVar</th>\n      <th>PreMax</th>\n      <th>PreMin</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td></td>\n      <td>4.031129</td>\n      <td>5.0</td>\n      <td>3</td>\n      <td>0.263204</td>\n      <td>0.296761</td>\n      <td>-0.746812</td>\n      <td>-0.953980</td>\n      <td>0.153360</td>\n      <td>0.343680</td>\n      <td>...</td>\n      <td>-0.774955</td>\n      <td>-0.954829</td>\n      <td>-0.167063</td>\n      <td>-0.119677</td>\n      <td>0.344867</td>\n      <td>0.344867</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>0.344867</td>\n      <td>0.344867</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>a</td>\n      <td>0.000000</td>\n      <td>1.5</td>\n      <td>1</td>\n      <td>-0.158803</td>\n      <td>-0.102414</td>\n      <td>-0.562209</td>\n      <td>-0.852578</td>\n      <td>-0.165842</td>\n      <td>-0.113848</td>\n      <td>...</td>\n      <td>-0.673249</td>\n      <td>-0.907875</td>\n      <td>0.282449</td>\n      <td>0.283081</td>\n      <td>0.374189</td>\n      <td>0.374189</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>0.374189</td>\n      <td>0.374189</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>b</td>\n      <td>4.609772</td>\n      <td>6.0</td>\n      <td>2</td>\n      <td>0.259733</td>\n      <td>0.294619</td>\n      <td>-0.751112</td>\n      <td>-0.953407</td>\n      <td>0.142498</td>\n      <td>0.344963</td>\n      <td>...</td>\n      <td>-0.827397</td>\n      <td>-0.969894</td>\n      <td>-0.172722</td>\n      <td>-0.112642</td>\n      <td>0.331312</td>\n      <td>0.331312</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>0.331312</td>\n      <td>0.331312</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>c</td>\n      <td>2.692582</td>\n      <td>4.0</td>\n      <td>2</td>\n      <td>0.176625</td>\n      <td>0.217678</td>\n      <td>-0.701008</td>\n      <td>-0.926940</td>\n      <td>0.089153</td>\n      <td>0.239879</td>\n      <td>...</td>\n      <td>-0.789811</td>\n      <td>-0.959105</td>\n      <td>-0.081256</td>\n      <td>-0.038746</td>\n      <td>0.361868</td>\n      <td>0.361868</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>0.361868</td>\n      <td>0.361868</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>d</td>\n      <td>2.000000</td>\n      <td>3.5</td>\n      <td>1</td>\n      <td>0.066985</td>\n      <td>0.114844</td>\n      <td>-0.686925</td>\n      <td>-0.921824</td>\n      <td>-0.008257</td>\n      <td>0.134961</td>\n      <td>...</td>\n      <td>-0.754170</td>\n      <td>-0.944572</td>\n      <td>0.036505</td>\n      <td>0.063000</td>\n      <td>0.371255</td>\n      <td>0.371255</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>0.371255</td>\n      <td>0.371255</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>e</td>\n      <td>1.802776</td>\n      <td>3.0</td>\n      <td>0</td>\n      <td>-0.023094</td>\n      <td>0.028453</td>\n      <td>-0.655597</td>\n      <td>-0.903264</td>\n      <td>-0.083387</td>\n      <td>0.042347</td>\n      <td>...</td>\n      <td>-0.721175</td>\n      <td>-0.929613</td>\n      <td>0.131097</td>\n      <td>0.146652</td>\n      <td>0.342579</td>\n      <td>0.342579</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>0.342579</td>\n      <td>0.342579</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>f</td>\n      <td>3.000000</td>\n      <td>4.5</td>\n      <td>1</td>\n      <td>0.108786</td>\n      <td>0.152744</td>\n      <td>-0.727190</td>\n      <td>-0.938727</td>\n      <td>0.012964</td>\n      <td>0.192386</td>\n      <td>...</td>\n      <td>-0.774333</td>\n      <td>-0.950961</td>\n      <td>-0.018478</td>\n      <td>0.018350</td>\n      <td>0.351042</td>\n      <td>0.351042</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>0.351042</td>\n      <td>0.351042</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>g</td>\n      <td>4.000000</td>\n      <td>5.5</td>\n      <td>1</td>\n      <td>0.147901</td>\n      <td>0.191665</td>\n      <td>-0.720540</td>\n      <td>-0.940661</td>\n      <td>0.052033</td>\n      <td>0.223479</td>\n      <td>...</td>\n      <td>-0.791777</td>\n      <td>-0.958728</td>\n      <td>-0.042781</td>\n      <td>0.000280</td>\n      <td>0.374867</td>\n      <td>0.374867</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>0.374867</td>\n      <td>0.374867</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>h</td>\n      <td>5.000000</td>\n      <td>6.5</td>\n      <td>1</td>\n      <td>0.241819</td>\n      <td>0.277230</td>\n      <td>-0.748850</td>\n      <td>-0.951136</td>\n      <td>0.126884</td>\n      <td>0.324900</td>\n      <td>...</td>\n      <td>-0.816898</td>\n      <td>-0.968289</td>\n      <td>-0.146433</td>\n      <td>-0.090606</td>\n      <td>0.378174</td>\n      <td>0.378174</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>0.378174</td>\n      <td>0.378174</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>i</td>\n      <td>6.576473</td>\n      <td>8.0</td>\n      <td>0</td>\n      <td>0.255855</td>\n      <td>0.290744</td>\n      <td>-0.740584</td>\n      <td>-0.949152</td>\n      <td>0.147189</td>\n      <td>0.335956</td>\n      <td>...</td>\n      <td>-0.816679</td>\n      <td>-0.967333</td>\n      <td>-0.174606</td>\n      <td>-0.119239</td>\n      <td>0.362228</td>\n      <td>0.362228</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>0.362228</td>\n      <td>0.362228</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>j</td>\n      <td>6.000000</td>\n      <td>7.5</td>\n      <td>1</td>\n      <td>0.272473</td>\n      <td>0.309670</td>\n      <td>-0.755372</td>\n      <td>-0.956453</td>\n      <td>0.149772</td>\n      <td>0.357694</td>\n      <td>...</td>\n      <td>-0.833686</td>\n      <td>-0.973050</td>\n      <td>-0.186007</td>\n      <td>-0.125157</td>\n      <td>0.405134</td>\n      <td>0.405134</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>0.405134</td>\n      <td>0.405134</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>k</td>\n      <td>7.000000</td>\n      <td>8.5</td>\n      <td>1</td>\n      <td>0.339304</td>\n      <td>0.369111</td>\n      <td>-0.727644</td>\n      <td>-0.945306</td>\n      <td>0.225544</td>\n      <td>0.408772</td>\n      <td>...</td>\n      <td>-0.800661</td>\n      <td>-0.954912</td>\n      <td>-0.262451</td>\n      <td>-0.206437</td>\n      <td>0.376719</td>\n      <td>0.376719</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>0.376719</td>\n      <td>0.376719</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>l</td>\n      <td>8.000000</td>\n      <td>9.5</td>\n      <td>1</td>\n      <td>0.382233</td>\n      <td>0.405073</td>\n      <td>-0.638259</td>\n      <td>-0.906771</td>\n      <td>0.306763</td>\n      <td>0.417367</td>\n      <td>...</td>\n      <td>-0.745010</td>\n      <td>-0.936543</td>\n      <td>-0.322365</td>\n      <td>-0.275522</td>\n      <td>0.319872</td>\n      <td>0.319872</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>0.319872</td>\n      <td>0.319872</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>m</td>\n      <td>6.576473</td>\n      <td>8.0</td>\n      <td>2</td>\n      <td>0.389207</td>\n      <td>0.418690</td>\n      <td>-0.707943</td>\n      <td>-0.937793</td>\n      <td>0.278708</td>\n      <td>0.442163</td>\n      <td>...</td>\n      <td>-0.805700</td>\n      <td>-0.961176</td>\n      <td>-0.321530</td>\n      <td>-0.262477</td>\n      <td>0.388432</td>\n      <td>0.388432</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>0.388432</td>\n      <td>0.388432</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>n</td>\n      <td>5.590170</td>\n      <td>7.0</td>\n      <td>2</td>\n      <td>0.331634</td>\n      <td>0.362837</td>\n      <td>-0.743480</td>\n      <td>-0.951985</td>\n      <td>0.213202</td>\n      <td>0.403625</td>\n      <td>...</td>\n      <td>-0.804161</td>\n      <td>-0.960631</td>\n      <td>-0.251065</td>\n      <td>-0.193773</td>\n      <td>0.382777</td>\n      <td>0.382777</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>0.382777</td>\n      <td>0.382777</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>o</td>\n      <td>7.566373</td>\n      <td>9.0</td>\n      <td>0</td>\n      <td>0.286624</td>\n      <td>0.319884</td>\n      <td>-0.733715</td>\n      <td>-0.947680</td>\n      <td>0.179723</td>\n      <td>0.362403</td>\n      <td>...</td>\n      <td>-0.785088</td>\n      <td>-0.958472</td>\n      <td>-0.213353</td>\n      <td>-0.162598</td>\n      <td>0.351025</td>\n      <td>0.351025</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>0.351025</td>\n      <td>0.351025</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>p</td>\n      <td>8.558621</td>\n      <td>10.0</td>\n      <td>0</td>\n      <td>0.371898</td>\n      <td>0.392885</td>\n      <td>-0.601324</td>\n      <td>-0.892655</td>\n      <td>0.319657</td>\n      <td>0.391443</td>\n      <td>...</td>\n      <td>-0.697584</td>\n      <td>-0.923587</td>\n      <td>-0.308737</td>\n      <td>-0.274732</td>\n      <td>0.336231</td>\n      <td>0.336231</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>0.336231</td>\n      <td>0.336231</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>q</td>\n      <td>1.118034</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>-0.264526</td>\n      <td>-0.198774</td>\n      <td>-0.531499</td>\n      <td>-0.839556</td>\n      <td>-0.256962</td>\n      <td>-0.221133</td>\n      <td>...</td>\n      <td>-0.645388</td>\n      <td>-0.888733</td>\n      <td>0.401593</td>\n      <td>0.395122</td>\n      <td>0.344465</td>\n      <td>0.344465</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>0.344465</td>\n      <td>0.344465</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>r</td>\n      <td>2.692582</td>\n      <td>4.0</td>\n      <td>0</td>\n      <td>0.069555</td>\n      <td>0.116442</td>\n      <td>-0.704776</td>\n      <td>-0.931194</td>\n      <td>-0.012913</td>\n      <td>0.141925</td>\n      <td>...</td>\n      <td>-0.747409</td>\n      <td>-0.943475</td>\n      <td>0.035223</td>\n      <td>0.063408</td>\n      <td>0.352901</td>\n      <td>0.352901</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>0.352901</td>\n      <td>0.352901</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>s</td>\n      <td>1.000000</td>\n      <td>2.5</td>\n      <td>1</td>\n      <td>-0.006690</td>\n      <td>0.040577</td>\n      <td>-0.628584</td>\n      <td>-0.887011</td>\n      <td>-0.051007</td>\n      <td>0.048894</td>\n      <td>...</td>\n      <td>-0.738408</td>\n      <td>-0.937212</td>\n      <td>0.113424</td>\n      <td>0.131729</td>\n      <td>0.348505</td>\n      <td>0.348505</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>0.348505</td>\n      <td>0.348505</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>t</td>\n      <td>3.640055</td>\n      <td>5.0</td>\n      <td>0</td>\n      <td>0.116050</td>\n      <td>0.163092</td>\n      <td>-0.724649</td>\n      <td>-0.940552</td>\n      <td>0.018509</td>\n      <td>0.190351</td>\n      <td>...</td>\n      <td>-0.772339</td>\n      <td>-0.952973</td>\n      <td>-0.012277</td>\n      <td>0.025682</td>\n      <td>0.325021</td>\n      <td>0.325021</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>0.325021</td>\n      <td>0.325021</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>u</td>\n      <td>5.590170</td>\n      <td>7.0</td>\n      <td>0</td>\n      <td>0.217092</td>\n      <td>0.254366</td>\n      <td>-0.722287</td>\n      <td>-0.941304</td>\n      <td>0.113238</td>\n      <td>0.293188</td>\n      <td>...</td>\n      <td>-0.791973</td>\n      <td>-0.949892</td>\n      <td>-0.126995</td>\n      <td>-0.076215</td>\n      <td>0.311063</td>\n      <td>0.311063</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>0.311063</td>\n      <td>0.311063</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>v</td>\n      <td>3.640055</td>\n      <td>5.0</td>\n      <td>2</td>\n      <td>0.216043</td>\n      <td>0.255153</td>\n      <td>-0.725401</td>\n      <td>-0.945603</td>\n      <td>0.113690</td>\n      <td>0.291929</td>\n      <td>...</td>\n      <td>-0.823008</td>\n      <td>-0.971734</td>\n      <td>-0.123508</td>\n      <td>-0.068384</td>\n      <td>0.377762</td>\n      <td>0.377762</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>0.377762</td>\n      <td>0.377762</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>w</td>\n      <td>1.118034</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>-0.123910</td>\n      <td>-0.070142</td>\n      <td>-0.526372</td>\n      <td>-0.828295</td>\n      <td>-0.125490</td>\n      <td>-0.095749</td>\n      <td>...</td>\n      <td>-0.659208</td>\n      <td>-0.906468</td>\n      <td>0.242984</td>\n      <td>0.242556</td>\n      <td>0.373852</td>\n      <td>0.373852</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>0.373852</td>\n      <td>0.373852</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>x</td>\n      <td>1.802776</td>\n      <td>3.0</td>\n      <td>2</td>\n      <td>0.118531</td>\n      <td>0.163655</td>\n      <td>-0.692257</td>\n      <td>-0.921865</td>\n      <td>0.037162</td>\n      <td>0.184497</td>\n      <td>...</td>\n      <td>-0.766687</td>\n      <td>-0.950972</td>\n      <td>-0.012482</td>\n      <td>0.022255</td>\n      <td>0.415190</td>\n      <td>0.415190</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>0.415190</td>\n      <td>0.415190</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>y</td>\n      <td>4.609772</td>\n      <td>6.0</td>\n      <td>0</td>\n      <td>0.171649</td>\n      <td>0.210967</td>\n      <td>-0.737031</td>\n      <td>-0.944728</td>\n      <td>0.064431</td>\n      <td>0.256416</td>\n      <td>...</td>\n      <td>-0.820402</td>\n      <td>-0.966527</td>\n      <td>-0.077335</td>\n      <td>-0.022922</td>\n      <td>0.332406</td>\n      <td>0.332406</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>0.332406</td>\n      <td>0.332406</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>z</td>\n      <td>1.118034</td>\n      <td>2.0</td>\n      <td>2</td>\n      <td>0.023697</td>\n      <td>0.075727</td>\n      <td>-0.653561</td>\n      <td>-0.901157</td>\n      <td>-0.035312</td>\n      <td>0.082856</td>\n      <td>...</td>\n      <td>-0.751571</td>\n      <td>-0.944376</td>\n      <td>0.091066</td>\n      <td>0.115010</td>\n      <td>0.365227</td>\n      <td>0.365227</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>0.365227</td>\n      <td>0.365227</td>\n    </tr>\n  </tbody>\n</table>\n<p>27 rows Ã— 280 columns</p>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group dataset by letter/distance\n",
    "\n",
    "# mean = mean of the rows with the same View and distances\n",
    "df_mean = df.groupby(['View','dist','x','y'], as_index=False).agg('mean')\n",
    "df_mean"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0 3.0\n",
      "5.0 3.5\n",
      "5.0 4.0\n",
      "5.0 4.5\n",
      "5.0 5.0\n",
      "5.0 5.5\n",
      "5.0 6.0\n",
      "5.0 6.5\n",
      "5.0 7.0\n"
     ]
    },
    {
     "data": {
      "text/plain": "   View      dist     x  y    AxMean     AxMed     AxStd     AxVar     AxMax  \\\n0        4.031129   5.0  3  0.263204  0.296761 -0.746812 -0.953980  0.153360   \n1     a  0.000000   1.5  1 -0.158803 -0.102414 -0.562209 -0.852578 -0.165842   \n2     b  4.609772   6.0  2  0.259733  0.294619 -0.751112 -0.953407  0.142498   \n3     c  2.692582   4.0  2  0.176625  0.217678 -0.701008 -0.926940  0.089153   \n4     d  2.000000   3.5  1  0.066985  0.114844 -0.686925 -0.921824 -0.008257   \n5     e  1.802776   3.0  0 -0.023094  0.028453 -0.655597 -0.903264 -0.083387   \n6     f  3.000000   4.5  1  0.108786  0.152744 -0.727190 -0.938727  0.012964   \n7     g  4.000000   5.5  1  0.147901  0.191665 -0.720540 -0.940661  0.052033   \n8     h  5.000000   6.5  1  0.241819  0.277230 -0.748850 -0.951136  0.126884   \n9     i  6.576473   8.0  0  0.255855  0.290744 -0.740584 -0.949152  0.147189   \n10    j  6.000000   7.5  1  0.272473  0.309670 -0.755372 -0.956453  0.149772   \n11    k  7.000000   8.5  1  0.339304  0.369111 -0.727644 -0.945306  0.225544   \n12    l  8.000000   9.5  1  0.382233  0.405073 -0.638259 -0.906771  0.306763   \n13    m  6.576473   8.0  2  0.389207  0.418690 -0.707943 -0.937793  0.278708   \n14    n  5.590170   7.0  2  0.331634  0.362837 -0.743480 -0.951985  0.213202   \n15    o  7.566373   9.0  0  0.286624  0.319884 -0.733715 -0.947680  0.179723   \n16    p  8.558621  10.0  0  0.371898  0.392885 -0.601324 -0.892655  0.319657   \n17    q  1.118034   1.0  0 -0.264526 -0.198774 -0.531499 -0.839556 -0.256962   \n18    r  2.692582   4.0  0  0.069555  0.116442 -0.704776 -0.931194 -0.012913   \n19    s  1.000000   2.5  1 -0.006690  0.040577 -0.628584 -0.887011 -0.051007   \n20    t  3.640055   5.0  0  0.116050  0.163092 -0.724649 -0.940552  0.018509   \n21    u  5.590170   7.0  0  0.217092  0.254366 -0.722287 -0.941304  0.113238   \n22    v  3.640055   5.0  2  0.216043  0.255153 -0.725401 -0.945603  0.113690   \n23    w  1.118034   2.0  0 -0.123910 -0.070142 -0.526372 -0.828295 -0.125490   \n24    x  1.802776   3.0  2  0.118531  0.163655 -0.692257 -0.921865  0.037162   \n25    y  4.609772   6.0  0  0.171649  0.210967 -0.737031 -0.944728  0.064431   \n26    z  1.118034   2.0  2  0.023697  0.075727 -0.653561 -0.901157 -0.035312   \n27       4.031129   3.0  3  0.263204  0.296761 -0.746812 -0.953980  0.153360   \n28       4.031129   3.5  3  0.263204  0.296761 -0.746812 -0.953980  0.153360   \n29       4.031129   4.0  3  0.263204  0.296761 -0.746812 -0.953980  0.153360   \n30       4.031129   4.5  3  0.263204  0.296761 -0.746812 -0.953980  0.153360   \n31       4.031129   5.5  3  0.263204  0.296761 -0.746812 -0.953980  0.153360   \n32       4.031129   6.0  3  0.263204  0.296761 -0.746812 -0.953980  0.153360   \n33       4.031129   6.5  3  0.263204  0.296761 -0.746812 -0.953980  0.153360   \n34       4.031129   7.0  3  0.263204  0.296761 -0.746812 -0.953980  0.153360   \n\n       AxMin  ...  GrPitchStd  GrPitchVar  GrPitchMax  GrPitchMin   PreMean  \\\n0   0.343680  ...   -0.774955   -0.954829   -0.167063   -0.119677  0.344867   \n1  -0.113848  ...   -0.673249   -0.907875    0.282449    0.283081  0.374189   \n2   0.344963  ...   -0.827397   -0.969894   -0.172722   -0.112642  0.331312   \n3   0.239879  ...   -0.789811   -0.959105   -0.081256   -0.038746  0.361868   \n4   0.134961  ...   -0.754170   -0.944572    0.036505    0.063000  0.371255   \n5   0.042347  ...   -0.721175   -0.929613    0.131097    0.146652  0.342579   \n6   0.192386  ...   -0.774333   -0.950961   -0.018478    0.018350  0.351042   \n7   0.223479  ...   -0.791777   -0.958728   -0.042781    0.000280  0.374867   \n8   0.324900  ...   -0.816898   -0.968289   -0.146433   -0.090606  0.378174   \n9   0.335956  ...   -0.816679   -0.967333   -0.174606   -0.119239  0.362228   \n10  0.357694  ...   -0.833686   -0.973050   -0.186007   -0.125157  0.405134   \n11  0.408772  ...   -0.800661   -0.954912   -0.262451   -0.206437  0.376719   \n12  0.417367  ...   -0.745010   -0.936543   -0.322365   -0.275522  0.319872   \n13  0.442163  ...   -0.805700   -0.961176   -0.321530   -0.262477  0.388432   \n14  0.403625  ...   -0.804161   -0.960631   -0.251065   -0.193773  0.382777   \n15  0.362403  ...   -0.785088   -0.958472   -0.213353   -0.162598  0.351025   \n16  0.391443  ...   -0.697584   -0.923587   -0.308737   -0.274732  0.336231   \n17 -0.221133  ...   -0.645388   -0.888733    0.401593    0.395122  0.344465   \n18  0.141925  ...   -0.747409   -0.943475    0.035223    0.063408  0.352901   \n19  0.048894  ...   -0.738408   -0.937212    0.113424    0.131729  0.348505   \n20  0.190351  ...   -0.772339   -0.952973   -0.012277    0.025682  0.325021   \n21  0.293188  ...   -0.791973   -0.949892   -0.126995   -0.076215  0.311063   \n22  0.291929  ...   -0.823008   -0.971734   -0.123508   -0.068384  0.377762   \n23 -0.095749  ...   -0.659208   -0.906468    0.242984    0.242556  0.373852   \n24  0.184497  ...   -0.766687   -0.950972   -0.012482    0.022255  0.415190   \n25  0.256416  ...   -0.820402   -0.966527   -0.077335   -0.022922  0.332406   \n26  0.082856  ...   -0.751571   -0.944376    0.091066    0.115010  0.365227   \n27  0.343680  ...   -0.774955   -0.954829   -0.167063   -0.119677  0.344867   \n28  0.343680  ...   -0.774955   -0.954829   -0.167063   -0.119677  0.344867   \n29  0.343680  ...   -0.774955   -0.954829   -0.167063   -0.119677  0.344867   \n30  0.343680  ...   -0.774955   -0.954829   -0.167063   -0.119677  0.344867   \n31  0.343680  ...   -0.774955   -0.954829   -0.167063   -0.119677  0.344867   \n32  0.343680  ...   -0.774955   -0.954829   -0.167063   -0.119677  0.344867   \n33  0.343680  ...   -0.774955   -0.954829   -0.167063   -0.119677  0.344867   \n34  0.343680  ...   -0.774955   -0.954829   -0.167063   -0.119677  0.344867   \n\n      PreMed  PreStd  PreVar    PreMax    PreMin  \n0   0.344867    -1.0    -1.0  0.344867  0.344867  \n1   0.374189    -1.0    -1.0  0.374189  0.374189  \n2   0.331312    -1.0    -1.0  0.331312  0.331312  \n3   0.361868    -1.0    -1.0  0.361868  0.361868  \n4   0.371255    -1.0    -1.0  0.371255  0.371255  \n5   0.342579    -1.0    -1.0  0.342579  0.342579  \n6   0.351042    -1.0    -1.0  0.351042  0.351042  \n7   0.374867    -1.0    -1.0  0.374867  0.374867  \n8   0.378174    -1.0    -1.0  0.378174  0.378174  \n9   0.362228    -1.0    -1.0  0.362228  0.362228  \n10  0.405134    -1.0    -1.0  0.405134  0.405134  \n11  0.376719    -1.0    -1.0  0.376719  0.376719  \n12  0.319872    -1.0    -1.0  0.319872  0.319872  \n13  0.388432    -1.0    -1.0  0.388432  0.388432  \n14  0.382777    -1.0    -1.0  0.382777  0.382777  \n15  0.351025    -1.0    -1.0  0.351025  0.351025  \n16  0.336231    -1.0    -1.0  0.336231  0.336231  \n17  0.344465    -1.0    -1.0  0.344465  0.344465  \n18  0.352901    -1.0    -1.0  0.352901  0.352901  \n19  0.348505    -1.0    -1.0  0.348505  0.348505  \n20  0.325021    -1.0    -1.0  0.325021  0.325021  \n21  0.311063    -1.0    -1.0  0.311063  0.311063  \n22  0.377762    -1.0    -1.0  0.377762  0.377762  \n23  0.373852    -1.0    -1.0  0.373852  0.373852  \n24  0.415190    -1.0    -1.0  0.415190  0.415190  \n25  0.332406    -1.0    -1.0  0.332406  0.332406  \n26  0.365227    -1.0    -1.0  0.365227  0.365227  \n27  0.344867    -1.0    -1.0  0.344867  0.344867  \n28  0.344867    -1.0    -1.0  0.344867  0.344867  \n29  0.344867    -1.0    -1.0  0.344867  0.344867  \n30  0.344867    -1.0    -1.0  0.344867  0.344867  \n31  0.344867    -1.0    -1.0  0.344867  0.344867  \n32  0.344867    -1.0    -1.0  0.344867  0.344867  \n33  0.344867    -1.0    -1.0  0.344867  0.344867  \n34  0.344867    -1.0    -1.0  0.344867  0.344867  \n\n[35 rows x 280 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>View</th>\n      <th>dist</th>\n      <th>x</th>\n      <th>y</th>\n      <th>AxMean</th>\n      <th>AxMed</th>\n      <th>AxStd</th>\n      <th>AxVar</th>\n      <th>AxMax</th>\n      <th>AxMin</th>\n      <th>...</th>\n      <th>GrPitchStd</th>\n      <th>GrPitchVar</th>\n      <th>GrPitchMax</th>\n      <th>GrPitchMin</th>\n      <th>PreMean</th>\n      <th>PreMed</th>\n      <th>PreStd</th>\n      <th>PreVar</th>\n      <th>PreMax</th>\n      <th>PreMin</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td></td>\n      <td>4.031129</td>\n      <td>5.0</td>\n      <td>3</td>\n      <td>0.263204</td>\n      <td>0.296761</td>\n      <td>-0.746812</td>\n      <td>-0.953980</td>\n      <td>0.153360</td>\n      <td>0.343680</td>\n      <td>...</td>\n      <td>-0.774955</td>\n      <td>-0.954829</td>\n      <td>-0.167063</td>\n      <td>-0.119677</td>\n      <td>0.344867</td>\n      <td>0.344867</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>0.344867</td>\n      <td>0.344867</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>a</td>\n      <td>0.000000</td>\n      <td>1.5</td>\n      <td>1</td>\n      <td>-0.158803</td>\n      <td>-0.102414</td>\n      <td>-0.562209</td>\n      <td>-0.852578</td>\n      <td>-0.165842</td>\n      <td>-0.113848</td>\n      <td>...</td>\n      <td>-0.673249</td>\n      <td>-0.907875</td>\n      <td>0.282449</td>\n      <td>0.283081</td>\n      <td>0.374189</td>\n      <td>0.374189</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>0.374189</td>\n      <td>0.374189</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>b</td>\n      <td>4.609772</td>\n      <td>6.0</td>\n      <td>2</td>\n      <td>0.259733</td>\n      <td>0.294619</td>\n      <td>-0.751112</td>\n      <td>-0.953407</td>\n      <td>0.142498</td>\n      <td>0.344963</td>\n      <td>...</td>\n      <td>-0.827397</td>\n      <td>-0.969894</td>\n      <td>-0.172722</td>\n      <td>-0.112642</td>\n      <td>0.331312</td>\n      <td>0.331312</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>0.331312</td>\n      <td>0.331312</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>c</td>\n      <td>2.692582</td>\n      <td>4.0</td>\n      <td>2</td>\n      <td>0.176625</td>\n      <td>0.217678</td>\n      <td>-0.701008</td>\n      <td>-0.926940</td>\n      <td>0.089153</td>\n      <td>0.239879</td>\n      <td>...</td>\n      <td>-0.789811</td>\n      <td>-0.959105</td>\n      <td>-0.081256</td>\n      <td>-0.038746</td>\n      <td>0.361868</td>\n      <td>0.361868</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>0.361868</td>\n      <td>0.361868</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>d</td>\n      <td>2.000000</td>\n      <td>3.5</td>\n      <td>1</td>\n      <td>0.066985</td>\n      <td>0.114844</td>\n      <td>-0.686925</td>\n      <td>-0.921824</td>\n      <td>-0.008257</td>\n      <td>0.134961</td>\n      <td>...</td>\n      <td>-0.754170</td>\n      <td>-0.944572</td>\n      <td>0.036505</td>\n      <td>0.063000</td>\n      <td>0.371255</td>\n      <td>0.371255</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>0.371255</td>\n      <td>0.371255</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>e</td>\n      <td>1.802776</td>\n      <td>3.0</td>\n      <td>0</td>\n      <td>-0.023094</td>\n      <td>0.028453</td>\n      <td>-0.655597</td>\n      <td>-0.903264</td>\n      <td>-0.083387</td>\n      <td>0.042347</td>\n      <td>...</td>\n      <td>-0.721175</td>\n      <td>-0.929613</td>\n      <td>0.131097</td>\n      <td>0.146652</td>\n      <td>0.342579</td>\n      <td>0.342579</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>0.342579</td>\n      <td>0.342579</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>f</td>\n      <td>3.000000</td>\n      <td>4.5</td>\n      <td>1</td>\n      <td>0.108786</td>\n      <td>0.152744</td>\n      <td>-0.727190</td>\n      <td>-0.938727</td>\n      <td>0.012964</td>\n      <td>0.192386</td>\n      <td>...</td>\n      <td>-0.774333</td>\n      <td>-0.950961</td>\n      <td>-0.018478</td>\n      <td>0.018350</td>\n      <td>0.351042</td>\n      <td>0.351042</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>0.351042</td>\n      <td>0.351042</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>g</td>\n      <td>4.000000</td>\n      <td>5.5</td>\n      <td>1</td>\n      <td>0.147901</td>\n      <td>0.191665</td>\n      <td>-0.720540</td>\n      <td>-0.940661</td>\n      <td>0.052033</td>\n      <td>0.223479</td>\n      <td>...</td>\n      <td>-0.791777</td>\n      <td>-0.958728</td>\n      <td>-0.042781</td>\n      <td>0.000280</td>\n      <td>0.374867</td>\n      <td>0.374867</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>0.374867</td>\n      <td>0.374867</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>h</td>\n      <td>5.000000</td>\n      <td>6.5</td>\n      <td>1</td>\n      <td>0.241819</td>\n      <td>0.277230</td>\n      <td>-0.748850</td>\n      <td>-0.951136</td>\n      <td>0.126884</td>\n      <td>0.324900</td>\n      <td>...</td>\n      <td>-0.816898</td>\n      <td>-0.968289</td>\n      <td>-0.146433</td>\n      <td>-0.090606</td>\n      <td>0.378174</td>\n      <td>0.378174</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>0.378174</td>\n      <td>0.378174</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>i</td>\n      <td>6.576473</td>\n      <td>8.0</td>\n      <td>0</td>\n      <td>0.255855</td>\n      <td>0.290744</td>\n      <td>-0.740584</td>\n      <td>-0.949152</td>\n      <td>0.147189</td>\n      <td>0.335956</td>\n      <td>...</td>\n      <td>-0.816679</td>\n      <td>-0.967333</td>\n      <td>-0.174606</td>\n      <td>-0.119239</td>\n      <td>0.362228</td>\n      <td>0.362228</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>0.362228</td>\n      <td>0.362228</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>j</td>\n      <td>6.000000</td>\n      <td>7.5</td>\n      <td>1</td>\n      <td>0.272473</td>\n      <td>0.309670</td>\n      <td>-0.755372</td>\n      <td>-0.956453</td>\n      <td>0.149772</td>\n      <td>0.357694</td>\n      <td>...</td>\n      <td>-0.833686</td>\n      <td>-0.973050</td>\n      <td>-0.186007</td>\n      <td>-0.125157</td>\n      <td>0.405134</td>\n      <td>0.405134</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>0.405134</td>\n      <td>0.405134</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>k</td>\n      <td>7.000000</td>\n      <td>8.5</td>\n      <td>1</td>\n      <td>0.339304</td>\n      <td>0.369111</td>\n      <td>-0.727644</td>\n      <td>-0.945306</td>\n      <td>0.225544</td>\n      <td>0.408772</td>\n      <td>...</td>\n      <td>-0.800661</td>\n      <td>-0.954912</td>\n      <td>-0.262451</td>\n      <td>-0.206437</td>\n      <td>0.376719</td>\n      <td>0.376719</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>0.376719</td>\n      <td>0.376719</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>l</td>\n      <td>8.000000</td>\n      <td>9.5</td>\n      <td>1</td>\n      <td>0.382233</td>\n      <td>0.405073</td>\n      <td>-0.638259</td>\n      <td>-0.906771</td>\n      <td>0.306763</td>\n      <td>0.417367</td>\n      <td>...</td>\n      <td>-0.745010</td>\n      <td>-0.936543</td>\n      <td>-0.322365</td>\n      <td>-0.275522</td>\n      <td>0.319872</td>\n      <td>0.319872</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>0.319872</td>\n      <td>0.319872</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>m</td>\n      <td>6.576473</td>\n      <td>8.0</td>\n      <td>2</td>\n      <td>0.389207</td>\n      <td>0.418690</td>\n      <td>-0.707943</td>\n      <td>-0.937793</td>\n      <td>0.278708</td>\n      <td>0.442163</td>\n      <td>...</td>\n      <td>-0.805700</td>\n      <td>-0.961176</td>\n      <td>-0.321530</td>\n      <td>-0.262477</td>\n      <td>0.388432</td>\n      <td>0.388432</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>0.388432</td>\n      <td>0.388432</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>n</td>\n      <td>5.590170</td>\n      <td>7.0</td>\n      <td>2</td>\n      <td>0.331634</td>\n      <td>0.362837</td>\n      <td>-0.743480</td>\n      <td>-0.951985</td>\n      <td>0.213202</td>\n      <td>0.403625</td>\n      <td>...</td>\n      <td>-0.804161</td>\n      <td>-0.960631</td>\n      <td>-0.251065</td>\n      <td>-0.193773</td>\n      <td>0.382777</td>\n      <td>0.382777</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>0.382777</td>\n      <td>0.382777</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>o</td>\n      <td>7.566373</td>\n      <td>9.0</td>\n      <td>0</td>\n      <td>0.286624</td>\n      <td>0.319884</td>\n      <td>-0.733715</td>\n      <td>-0.947680</td>\n      <td>0.179723</td>\n      <td>0.362403</td>\n      <td>...</td>\n      <td>-0.785088</td>\n      <td>-0.958472</td>\n      <td>-0.213353</td>\n      <td>-0.162598</td>\n      <td>0.351025</td>\n      <td>0.351025</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>0.351025</td>\n      <td>0.351025</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>p</td>\n      <td>8.558621</td>\n      <td>10.0</td>\n      <td>0</td>\n      <td>0.371898</td>\n      <td>0.392885</td>\n      <td>-0.601324</td>\n      <td>-0.892655</td>\n      <td>0.319657</td>\n      <td>0.391443</td>\n      <td>...</td>\n      <td>-0.697584</td>\n      <td>-0.923587</td>\n      <td>-0.308737</td>\n      <td>-0.274732</td>\n      <td>0.336231</td>\n      <td>0.336231</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>0.336231</td>\n      <td>0.336231</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>q</td>\n      <td>1.118034</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>-0.264526</td>\n      <td>-0.198774</td>\n      <td>-0.531499</td>\n      <td>-0.839556</td>\n      <td>-0.256962</td>\n      <td>-0.221133</td>\n      <td>...</td>\n      <td>-0.645388</td>\n      <td>-0.888733</td>\n      <td>0.401593</td>\n      <td>0.395122</td>\n      <td>0.344465</td>\n      <td>0.344465</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>0.344465</td>\n      <td>0.344465</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>r</td>\n      <td>2.692582</td>\n      <td>4.0</td>\n      <td>0</td>\n      <td>0.069555</td>\n      <td>0.116442</td>\n      <td>-0.704776</td>\n      <td>-0.931194</td>\n      <td>-0.012913</td>\n      <td>0.141925</td>\n      <td>...</td>\n      <td>-0.747409</td>\n      <td>-0.943475</td>\n      <td>0.035223</td>\n      <td>0.063408</td>\n      <td>0.352901</td>\n      <td>0.352901</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>0.352901</td>\n      <td>0.352901</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>s</td>\n      <td>1.000000</td>\n      <td>2.5</td>\n      <td>1</td>\n      <td>-0.006690</td>\n      <td>0.040577</td>\n      <td>-0.628584</td>\n      <td>-0.887011</td>\n      <td>-0.051007</td>\n      <td>0.048894</td>\n      <td>...</td>\n      <td>-0.738408</td>\n      <td>-0.937212</td>\n      <td>0.113424</td>\n      <td>0.131729</td>\n      <td>0.348505</td>\n      <td>0.348505</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>0.348505</td>\n      <td>0.348505</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>t</td>\n      <td>3.640055</td>\n      <td>5.0</td>\n      <td>0</td>\n      <td>0.116050</td>\n      <td>0.163092</td>\n      <td>-0.724649</td>\n      <td>-0.940552</td>\n      <td>0.018509</td>\n      <td>0.190351</td>\n      <td>...</td>\n      <td>-0.772339</td>\n      <td>-0.952973</td>\n      <td>-0.012277</td>\n      <td>0.025682</td>\n      <td>0.325021</td>\n      <td>0.325021</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>0.325021</td>\n      <td>0.325021</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>u</td>\n      <td>5.590170</td>\n      <td>7.0</td>\n      <td>0</td>\n      <td>0.217092</td>\n      <td>0.254366</td>\n      <td>-0.722287</td>\n      <td>-0.941304</td>\n      <td>0.113238</td>\n      <td>0.293188</td>\n      <td>...</td>\n      <td>-0.791973</td>\n      <td>-0.949892</td>\n      <td>-0.126995</td>\n      <td>-0.076215</td>\n      <td>0.311063</td>\n      <td>0.311063</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>0.311063</td>\n      <td>0.311063</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>v</td>\n      <td>3.640055</td>\n      <td>5.0</td>\n      <td>2</td>\n      <td>0.216043</td>\n      <td>0.255153</td>\n      <td>-0.725401</td>\n      <td>-0.945603</td>\n      <td>0.113690</td>\n      <td>0.291929</td>\n      <td>...</td>\n      <td>-0.823008</td>\n      <td>-0.971734</td>\n      <td>-0.123508</td>\n      <td>-0.068384</td>\n      <td>0.377762</td>\n      <td>0.377762</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>0.377762</td>\n      <td>0.377762</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>w</td>\n      <td>1.118034</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>-0.123910</td>\n      <td>-0.070142</td>\n      <td>-0.526372</td>\n      <td>-0.828295</td>\n      <td>-0.125490</td>\n      <td>-0.095749</td>\n      <td>...</td>\n      <td>-0.659208</td>\n      <td>-0.906468</td>\n      <td>0.242984</td>\n      <td>0.242556</td>\n      <td>0.373852</td>\n      <td>0.373852</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>0.373852</td>\n      <td>0.373852</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>x</td>\n      <td>1.802776</td>\n      <td>3.0</td>\n      <td>2</td>\n      <td>0.118531</td>\n      <td>0.163655</td>\n      <td>-0.692257</td>\n      <td>-0.921865</td>\n      <td>0.037162</td>\n      <td>0.184497</td>\n      <td>...</td>\n      <td>-0.766687</td>\n      <td>-0.950972</td>\n      <td>-0.012482</td>\n      <td>0.022255</td>\n      <td>0.415190</td>\n      <td>0.415190</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>0.415190</td>\n      <td>0.415190</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>y</td>\n      <td>4.609772</td>\n      <td>6.0</td>\n      <td>0</td>\n      <td>0.171649</td>\n      <td>0.210967</td>\n      <td>-0.737031</td>\n      <td>-0.944728</td>\n      <td>0.064431</td>\n      <td>0.256416</td>\n      <td>...</td>\n      <td>-0.820402</td>\n      <td>-0.966527</td>\n      <td>-0.077335</td>\n      <td>-0.022922</td>\n      <td>0.332406</td>\n      <td>0.332406</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>0.332406</td>\n      <td>0.332406</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>z</td>\n      <td>1.118034</td>\n      <td>2.0</td>\n      <td>2</td>\n      <td>0.023697</td>\n      <td>0.075727</td>\n      <td>-0.653561</td>\n      <td>-0.901157</td>\n      <td>-0.035312</td>\n      <td>0.082856</td>\n      <td>...</td>\n      <td>-0.751571</td>\n      <td>-0.944376</td>\n      <td>0.091066</td>\n      <td>0.115010</td>\n      <td>0.365227</td>\n      <td>0.365227</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>0.365227</td>\n      <td>0.365227</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td></td>\n      <td>4.031129</td>\n      <td>3.0</td>\n      <td>3</td>\n      <td>0.263204</td>\n      <td>0.296761</td>\n      <td>-0.746812</td>\n      <td>-0.953980</td>\n      <td>0.153360</td>\n      <td>0.343680</td>\n      <td>...</td>\n      <td>-0.774955</td>\n      <td>-0.954829</td>\n      <td>-0.167063</td>\n      <td>-0.119677</td>\n      <td>0.344867</td>\n      <td>0.344867</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>0.344867</td>\n      <td>0.344867</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td></td>\n      <td>4.031129</td>\n      <td>3.5</td>\n      <td>3</td>\n      <td>0.263204</td>\n      <td>0.296761</td>\n      <td>-0.746812</td>\n      <td>-0.953980</td>\n      <td>0.153360</td>\n      <td>0.343680</td>\n      <td>...</td>\n      <td>-0.774955</td>\n      <td>-0.954829</td>\n      <td>-0.167063</td>\n      <td>-0.119677</td>\n      <td>0.344867</td>\n      <td>0.344867</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>0.344867</td>\n      <td>0.344867</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td></td>\n      <td>4.031129</td>\n      <td>4.0</td>\n      <td>3</td>\n      <td>0.263204</td>\n      <td>0.296761</td>\n      <td>-0.746812</td>\n      <td>-0.953980</td>\n      <td>0.153360</td>\n      <td>0.343680</td>\n      <td>...</td>\n      <td>-0.774955</td>\n      <td>-0.954829</td>\n      <td>-0.167063</td>\n      <td>-0.119677</td>\n      <td>0.344867</td>\n      <td>0.344867</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>0.344867</td>\n      <td>0.344867</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td></td>\n      <td>4.031129</td>\n      <td>4.5</td>\n      <td>3</td>\n      <td>0.263204</td>\n      <td>0.296761</td>\n      <td>-0.746812</td>\n      <td>-0.953980</td>\n      <td>0.153360</td>\n      <td>0.343680</td>\n      <td>...</td>\n      <td>-0.774955</td>\n      <td>-0.954829</td>\n      <td>-0.167063</td>\n      <td>-0.119677</td>\n      <td>0.344867</td>\n      <td>0.344867</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>0.344867</td>\n      <td>0.344867</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td></td>\n      <td>4.031129</td>\n      <td>5.5</td>\n      <td>3</td>\n      <td>0.263204</td>\n      <td>0.296761</td>\n      <td>-0.746812</td>\n      <td>-0.953980</td>\n      <td>0.153360</td>\n      <td>0.343680</td>\n      <td>...</td>\n      <td>-0.774955</td>\n      <td>-0.954829</td>\n      <td>-0.167063</td>\n      <td>-0.119677</td>\n      <td>0.344867</td>\n      <td>0.344867</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>0.344867</td>\n      <td>0.344867</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td></td>\n      <td>4.031129</td>\n      <td>6.0</td>\n      <td>3</td>\n      <td>0.263204</td>\n      <td>0.296761</td>\n      <td>-0.746812</td>\n      <td>-0.953980</td>\n      <td>0.153360</td>\n      <td>0.343680</td>\n      <td>...</td>\n      <td>-0.774955</td>\n      <td>-0.954829</td>\n      <td>-0.167063</td>\n      <td>-0.119677</td>\n      <td>0.344867</td>\n      <td>0.344867</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>0.344867</td>\n      <td>0.344867</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td></td>\n      <td>4.031129</td>\n      <td>6.5</td>\n      <td>3</td>\n      <td>0.263204</td>\n      <td>0.296761</td>\n      <td>-0.746812</td>\n      <td>-0.953980</td>\n      <td>0.153360</td>\n      <td>0.343680</td>\n      <td>...</td>\n      <td>-0.774955</td>\n      <td>-0.954829</td>\n      <td>-0.167063</td>\n      <td>-0.119677</td>\n      <td>0.344867</td>\n      <td>0.344867</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>0.344867</td>\n      <td>0.344867</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td></td>\n      <td>4.031129</td>\n      <td>7.0</td>\n      <td>3</td>\n      <td>0.263204</td>\n      <td>0.296761</td>\n      <td>-0.746812</td>\n      <td>-0.953980</td>\n      <td>0.153360</td>\n      <td>0.343680</td>\n      <td>...</td>\n      <td>-0.774955</td>\n      <td>-0.954829</td>\n      <td>-0.167063</td>\n      <td>-0.119677</td>\n      <td>0.344867</td>\n      <td>0.344867</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>0.344867</td>\n      <td>0.344867</td>\n    </tr>\n  </tbody>\n</table>\n<p>35 rows Ã— 280 columns</p>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PREPARE A DATAFRAME FOR HEATMAPS: MULTIPLY SPACES TO SHOW MULTIPLE BLOCKS\n",
    "\n",
    "df_heat = df_mean.copy()\n",
    "\n",
    "# Duplicate spaces\n",
    "space_x = df_heat['x'][0]\n",
    "row = df_heat.iloc[[0],]\n",
    "SPACEBAR_LEN = 9\n",
    "BLOCK_SIZE = 0.5\n",
    "for i in range(SPACEBAR_LEN):\n",
    "    new_block = space_x - (SPACEBAR_LEN - 1) / 2 * BLOCK_SIZE + i * BLOCK_SIZE\n",
    "    print(space_x, new_block)\n",
    "    if new_block == space_x:\n",
    "        continue\n",
    "\n",
    "    # append the other rows for spacebar draw\n",
    "    df_heat = df_heat.append(df_heat.loc[[0] * 1].assign(**{'x': new_block}), ignore_index=True) # 1 is the number of repeats\n",
    "\n",
    "df_heat"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "<AxesSubplot:title={'center':'Title'}, xlabel='x', ylabel='y'>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 720x216 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#HEATMAP - INTERACTIVE\n",
    "\n",
    "labz = [\n",
    "    ['q','','w','','e','','r','','t','','y','','u','','i','','o','','p'],\n",
    "    ['','a','','s','','d','','f','','g','','h','','j','','k','','l',''],\n",
    "    ['','','z','','x','','c','','v','','b','','n','','m','','','',''],\n",
    "    ['','','','','','','','','space','','','','','','','','','','']\n",
    "]\n",
    "\n",
    "# in pivot: 'y' is the index of rows, 'x' index of columns and 'dist' the value\n",
    "_ = df_heat.pivot('y', 'x', 'dist') # <--- Change here 'dist' to the column you want\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.axis('off')\n",
    "plt.title(\"Title\")\n",
    "sns.heatmap(_, annot=labz, fmt='', square=True, cbar_kws = dict(use_gridspec=False,location=\"bottom\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "#HEATMAP - GEN FILES\n",
    "\n",
    "take_out = ['x', 'y', 'dist', 'View']\n",
    "\n",
    "for col in df_heat.columns:\n",
    "    if col in take_out:\n",
    "        continue\n",
    "    _ = df_heat.pivot('y', 'x', col) # col takes all values of letters\n",
    "    plt.figure(figsize=(15,8))\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Heatmap: \" + col)\n",
    "    sns.heatmap(_, annot=labz, fmt='', square=True, cbar_kws = dict(use_gridspec=False,location=\"bottom\"))\n",
    "    # plt.savefig(\"C:\\\\Users\\\\giuli\\\\Desktop\\\\ds\\\\merged\\\\png\\\\\" + col + \"_heat.png\")\n",
    "    plt.savefig(\"C:\\\\Users\\\\giuli\\\\Desktop\\\\image finale\\\\heatmap inizali\\\\\" + col + \"_heat.png\", bbox_inches='tight')\n",
    "    plt.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV\n",
    "from matplotlib import pyplot\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# compare machine learning algorithms\n",
    "\n",
    "models = []\n",
    "models.append(('RFC', \"RandomForestClassifier\", RandomForestClassifier(n_estimators = 100, random_state = 42)))\n",
    "models.append(('NB', \"GaussianNB\", GaussianNB()))\n",
    "models.append(('SVM', \"SVC\", SVC(C=1.0, kernel='linear', gamma='auto', random_state=42)))\n",
    "models.append(('CART', \"DecisionTreeClassifier\", DecisionTreeClassifier(random_state=42)))\n",
    "models.append(('ADA', \"AdaBoostClassifier\", AdaBoostClassifier(n_estimators=100, learning_rate=0.1)))\n",
    "models.append(('LR', \"LogisticRegression\", LogisticRegression(solver='liblinear', random_state = 42)))\n",
    "models.append(('KNN', \"KNeighborsClassifier\", KNeighborsClassifier(weights='distance', n_neighbors=30, n_jobs=-1)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25625, 276) (25625,)\n",
      "Train: 17937 17937 Test: 7688 7688\n",
      "{' ': 1228, 'a': 464, 'b': 109, 'c': 216, 'd': 213, 'e': 785, 'f': 127, 'g': 131, 'h': 232, 'i': 530, 'j': 104, 'k': 111, 'l': 261, 'm': 152, 'n': 352, 'o': 425, 'p': 187, 'q': 100, 'r': 295, 's': 328, 't': 431, 'u': 267, 'v': 101, 'w': 134, 'x': 104, 'y': 175, 'z': 126}\n",
      "RFC: 37.760000\n",
      "NB: 13.980000\n",
      "SVM: 37.930000\n",
      "CART: 23.390000\n",
      "ADA: 24.520000\n",
      "LR: 36.490000\n",
      "KNN: 34.160000\n"
     ]
    }
   ],
   "source": [
    "features = df.drop(['View','User','Hand', 'Smartphone', 'x', 'y','dist'], axis=1)\n",
    "feature_list = list(features.columns)\n",
    "\n",
    "# Convert to numpy array\n",
    "labels = np.array(df['View'])\n",
    "features = np.array(features)\n",
    "\n",
    "# print(features)\n",
    "# print(labels)\n",
    "\n",
    "print(features.shape, labels.shape)\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=0)\n",
    "for train_index, test_index in sss.split(features, labels):\n",
    "    X_train, X_test = features[train_index], features[test_index]\n",
    "    y_train, y_test = labels[train_index], labels[test_index]\n",
    "\n",
    "print(\"Train:\", len(X_train), len(y_train), \"Test:\", len(X_test), len(y_test))\n",
    "\n",
    "# print number of occurrences for each letters\n",
    "a, b = np.unique(y_test, return_counts=True)\n",
    "print(dict(zip(a, b)))\n",
    "\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "hue = []\n",
    "predictions = []\n",
    "\n",
    "y_pred_result = []\n",
    "\n",
    "for name, full_name, model in models:\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = round(metrics.accuracy_score(y_test, y_pred) * 100, 2)\n",
    "\n",
    "    y_pred_result.append(y_pred)\n",
    "    predictions.append((full_name, y_pred))\n",
    "    results.append(accuracy)\n",
    "    names.append(name)\n",
    "    hue.append(full_name + \": \" + str(accuracy) + \"%\")\n",
    "\n",
    "    print('%s: %f' % (name, accuracy))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# plot ACCURACY of ML Algorithm\n",
    "\n",
    "graphData = pd.DataFrame({'names':names,'results':results, 'algorithm':hue})\n",
    "fig = pyplot.figure(figsize=(24,14), dpi=100)\n",
    "pyplot.title('Accuracy', fontweight='bold')\n",
    "plt_bar = sns.barplot(x='names', y='results', data=graphData, hue='algorithm', dodge=False)\n",
    "pyplot.xlabel(\"algorithm\", fontsize=10)\n",
    "pyplot.ylabel(\"accuracy\", fontweight='bold', fontsize=10)\n",
    "plt.yticks(np.arange(0, 105, 5), fontsize=15)\n",
    "plt.xticks(fontsize=17)\n",
    "plt.setp(plt_bar.get_legend().get_texts(), fontsize='15') # for legend text\n",
    "plt.setp(plt_bar.get_legend().get_title(), fontsize='16', fontweight='bold') # for legend title\n",
    "pyplot.savefig(\"C:\\\\Users\\\\giuli\\\\Desktop\\\\image finale\\\\\" + \"accuracy.png\",\n",
    "               bbox_inches='tight')\n",
    "# pyplot.show()\n",
    "pyplot.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# mod = AdaBoostClassifier()\n",
    "# # define the grid of values to search\n",
    "# grid = dict()\n",
    "# grid['n_estimators'] = [10, 50, 100, 500]\n",
    "# grid['learning_rate'] = [0.0001, 0.001, 0.01, 0.1, 1.0]\n",
    "# grid['algorithm'] = ['SAMME', 'SAMME.R']\n",
    "# # define the evaluation procedure\n",
    "# # define the grid search procedure\n",
    "# grid_search = GridSearchCV(estimator=mod, param_grid=grid, n_jobs=-1, cv=sss, scoring='accuracy')\n",
    "# # execute the grid search\n",
    "# grid_result = grid_search.fit(X_train, y_train)\n",
    "# # summarize the best score and configuration\n",
    "# print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "# mod = SVC(probability=True, shrinking=False, gamma='auto', decision_function_shape='ovo', C=1.0, kernel='linear', random_state=42)\n",
    "# # define the grid of values to search\n",
    "# grid = dict()\n",
    "# # grid['C'] = [0.001, 0.01, 0.1, 1.0]\n",
    "# # grid['kernel'] = ['linear', 'rbf', 'sigmoid', 'poly']\n",
    "# # grid['degree'] = [3, 4, 5]\n",
    "# # grid['gamma'] = ['auto', 'scale']\n",
    "# # grid['shrinking'] = [True, False]\n",
    "# # grid['probability'] = [True, False]\n",
    "# # grid['decision_function_shape'] = ['ovo', 'ovr']\n",
    "# grid['break_ties'] = [True, False]\n",
    "# # define the evaluation procedure\n",
    "# # define the grid search procedure\n",
    "# grid_search = GridSearchCV(estimator=mod, param_grid=grid, n_jobs=-1, cv=sss, scoring='accuracy')\n",
    "# # execute the grid search\n",
    "# grid_result = grid_search.fit(X_train, y_train)\n",
    "# # summarize the best score and configuration\n",
    "# print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "# mod = LogisticRegression(random_state=42, multi_class='auto')\n",
    "# # define the grid of values to search\n",
    "# grid = dict()\n",
    "# grid['C'] = [0.001, 0.01, 0.1, 1.0]\n",
    "# grid['solver'] = ['linear', 'newton-cg', 'lbfgs', 'saga']\n",
    "# # define the evaluation procedure\n",
    "# # define the grid search procedure\n",
    "# grid_search = GridSearchCV(estimator=mod, param_grid=grid, n_jobs=-1, cv=sss, scoring='accuracy')\n",
    "# # execute the grid search\n",
    "# grid_result = grid_search.fit(X_train, y_train)\n",
    "# # summarize the best score and configuration\n",
    "# print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "# mod = DecisionTreeClassifier(random_state=42)\n",
    "# # define the grid of values to search\n",
    "# grid = dict()\n",
    "# grid['splitter'] = ['best', 'random']\n",
    "# grid['max_features'] = ['auto', 'sqrt', 'log2', None]\n",
    "# # define the evaluation procedure\n",
    "# # define the grid search procedure\n",
    "# grid_search = GridSearchCV(estimator=mod, param_grid=grid, n_jobs=-1, cv=sss, scoring='accuracy')\n",
    "# # execute the grid search\n",
    "# grid_result = grid_search.fit(X_train, y_train)\n",
    "# # summarize the best score and configuration\n",
    "# print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "# mod = KNeighborsClassifier(n_jobs=-1)\n",
    "# # define the grid of values to search\n",
    "# grid = dict()\n",
    "# grid['n_neighbors'] = [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n",
    "#                        21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]\n",
    "# grid['weights'] = ['uniform', 'distance']\n",
    "# grid['algorithm'] = ['auto', 'ball_tree', 'kd_tree', 'brute']\n",
    "# # define the evaluation procedure\n",
    "# # define the grid search procedure\n",
    "# grid_search = GridSearchCV(estimator=mod, param_grid=grid, n_jobs=-1, cv=sss, scoring='accuracy')\n",
    "# # execute the grid search\n",
    "# grid_result = grid_search.fit(X_train, y_train)\n",
    "# # summarize the best score and configuration\n",
    "# print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 1728x1008 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# mod = RandomForestClassifier(random_state=42)\n",
    "# # define the grid of values to search\n",
    "# grid = dict()\n",
    "# grid['n_estimators'] = [10, 50, 100, 500]\n",
    "# grid['min_samples_leaf'] = [1, 75]\n",
    "# # define the evaluation procedure\n",
    "# # define the grid search procedure\n",
    "# grid_search = GridSearchCV(estimator=mod, param_grid=grid, n_jobs=-1, cv=sss, scoring='accuracy')\n",
    "# # execute the grid search\n",
    "# grid_result = grid_search.fit(X_train, y_train)\n",
    "# # summarize the best score and configuration\n",
    "# print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report, f1_score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "\n",
    "idx = [17, 23, 1, 5, 19, 26, 18, 4, 24, 20, 6, 3, 25, 7, 22, 0, 21, 8, 2, 9, 10, 14, 15, 11, 13, 16, 12]\n",
    "\n",
    "labels_order = a[idx]\n",
    "\n",
    "ticklabels = [i for i in \"QWAESZRDXTFCYGV UHBIJNOKMPL\"]\n",
    "zip_labels = list(zip(idx, ticklabels))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier\n",
      "GaussianNB\n",
      "SVC\n",
      "DecisionTreeClassifier\n",
      "AdaBoostClassifier\n",
      "LogisticRegression\n",
      "KNeighborsClassifier\n"
     ]
    }
   ],
   "source": [
    "# plot CONFUSION MATRIX not scaled\n",
    "\n",
    "copy_conf = []\n",
    "\n",
    "for name, prediction in predictions:\n",
    "\n",
    "    c, d = np.unique(prediction, return_counts=True)\n",
    "    # print(dict(zip(c, d)))\n",
    "\n",
    "    conf_matrix = confusion_matrix(y_test, prediction, labels=labels_order)\n",
    "\n",
    "    copy_conf.append((name, conf_matrix))\n",
    "\n",
    "    print(name)\n",
    "\n",
    "    # number of letters for each rows\n",
    "    # print(name, list(map(sum, conf_matrix)))\n",
    "\n",
    "    # number of letters for each columns\n",
    "    # print(name, [sum(conf_matrix[:,i]) for i in range(27)])\n",
    "\n",
    "    df_cm = pd.DataFrame(conf_matrix, index = idx, columns = idx)\n",
    "\n",
    "    plt.figure(figsize = (24,14), dpi=100)\n",
    "    sns.set(font_scale=1)\n",
    "\n",
    "    sns.heatmap(df_cm, annot=True, yticklabels=zip_labels, xticklabels=ticklabels, annot_kws={\"fontsize\":10})\n",
    "\n",
    "    plt.title(\"confusion_matrix: \" + name, fontweight='bold')\n",
    "    plt.ylabel(\"actual\", fontweight='bold')\n",
    "    plt.xlabel(\"predicted\", fontweight='bold')\n",
    "\n",
    "    plt.savefig(\"C:\\\\Users\\\\giuli\\\\Desktop\\\\image finale\\\\confusion_matrix\\\\non scalati\\\\\" + name +\n",
    "                \"_confusion_matrix.png\", bbox_inches='tight')\n",
    "\n",
    "    plt.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# plot CONFUSION MATRIX scaled\n",
    "\n",
    "copy_conf_scaled = []\n",
    "\n",
    "for name, prediction in predictions:\n",
    "\n",
    "    c, d = np.unique(prediction, return_counts=True)\n",
    "    # print(dict(zip(c, d)))\n",
    "\n",
    "    conf_matrix = confusion_matrix(y_test, prediction, labels=labels_order)\n",
    "\n",
    "    conf_scaled = (conf_matrix.astype(float)/conf_matrix.astype(float).sum(axis=1)[:, np.newaxis]) * 100\n",
    "\n",
    "    copy_conf_scaled.append((name, conf_scaled))\n",
    "\n",
    "    # print(name, list(map(sum, conf_scaled)))\n",
    "\n",
    "    df_cm = pd.DataFrame(conf_scaled, index = idx, columns = idx)\n",
    "\n",
    "    plt.figure(figsize = (24,14), dpi=100)\n",
    "    sns.set(font_scale=1)\n",
    "\n",
    "    sns.heatmap(df_cm, annot=True, yticklabels=zip_labels, xticklabels=ticklabels, annot_kws={\"fontsize\":10})\n",
    "\n",
    "    plt.title(\"confusion_matrix: \" + name, fontweight='bold')\n",
    "    plt.ylabel(\"actual\", fontweight='bold')\n",
    "    plt.xlabel(\"predicted\", fontweight='bold')\n",
    "\n",
    "    plt.savefig(\"C:\\\\Users\\\\giuli\\\\Desktop\\\\image finale\\\\confusion_matrix\\\\scalati\\\\\" + name +\n",
    "                \"_confusion_matrix.png\", bbox_inches='tight')\n",
    "\n",
    "    plt.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\giuli\\AppData\\Local\\Temp/ipykernel_15932/1748257599.py:7: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  nArr2D = np.array(copy_conf)\n"
     ]
    }
   ],
   "source": [
    "# keyboard = [('q', 0), ('w', 1), ('a', 1), ('e', 2), ('s', 2), ('z', 2), ('r', 3), ('d', 3), ('x', 3), ('t', 4), ('f', 4),\n",
    "#             ('c', 4), ('y', 5), ('g', 5), ('v', 5), (' ', 5), ('u', 6), ('h', 6), ('b', 6), ('i', 7), ('j', 7), ('n', 7),\n",
    "#             ('o', 8), ('k', 8), ('m', 8), ('p', 9), ('l', 9)]\n",
    "\n",
    "# distances of each letter from the others\n",
    "\n",
    "nArr2D = np.array(copy_conf)\n",
    "keyboard_q = [0, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 4, 6, 6, 6, 7, 7, 7, 8, 8, 8, 9, 9]\n",
    "keyboard_w = [1, 0, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 4, 4, 4, 3, 5, 5, 5, 6, 6, 6, 7, 7, 7, 8, 8]\n",
    "keyboard_a = [1, 1, 0, 2, 1, 1, 3, 2, 2, 4, 3, 3, 5, 4, 4, 3, 6, 5, 5, 7, 6, 6, 8, 7, 7, 9, 8]\n",
    "keyboard_e = [2, 1, 2, 0, 1, 2, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6, 7, 7]\n",
    "keyboard_s = [2, 1, 1, 1, 0, 1, 2, 1, 1, 3, 2, 2, 4, 3, 3, 2, 5, 4, 4, 6, 5, 5, 7, 6, 6, 8, 7]\n",
    "keyboard_z = [2, 2, 1, 2, 1, 0, 2, 1, 1, 3, 2, 2, 4, 3, 3, 2, 5, 4, 4, 6, 5, 5, 7, 6, 6, 8, 7]\n",
    "keyboard_r = [3, 2, 3, 1, 2, 2, 0, 1, 2, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6]\n",
    "keyboard_d = [3, 2, 2, 1, 1, 1, 1, 0, 1, 2, 1, 1, 3, 2, 2, 2, 4, 3, 3, 5, 4, 4, 6, 5, 5, 7, 6]\n",
    "keyboard_x = [3, 2, 2, 2, 1, 1, 2, 1, 0, 2, 1, 1, 3, 2, 2, 1, 4, 3, 3, 5, 4, 4, 6, 5, 5, 7, 6]\n",
    "keyboard_t = [4, 3, 4, 2, 3, 3, 1, 2, 2, 0, 1, 2, 1, 1, 2, 3, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5]\n",
    "keyboard_f = [4, 3, 3, 2, 2, 2, 1, 1, 1, 1, 0, 1, 2, 1, 1, 2, 3, 2, 2, 4, 3, 3, 5, 4, 4, 6, 5]\n",
    "keyboard_c = [4, 3, 3, 2, 2, 2, 2, 1, 1, 2, 1, 0, 2, 1, 1, 1, 3, 2, 2, 4, 3, 3, 5, 4, 4, 6, 5]\n",
    "keyboard_y = [5, 4, 5, 3, 4, 4, 2, 3, 3, 1, 2, 2, 0, 1, 2, 3, 1, 1, 2, 2, 2, 2, 3, 3, 3, 4, 4]\n",
    "keyboard_g = [5, 4, 4, 3, 3, 3, 2, 2, 2, 1, 1, 1, 1, 0, 1, 2, 2, 1, 1, 3, 2, 2, 4, 3, 3, 5, 4]\n",
    "keyboard_v = [5, 4, 4, 3, 3, 3, 2, 2, 2, 2, 1, 1, 2, 1, 0, 1, 2, 1, 1, 3, 2, 2, 4, 3, 3, 5, 4]\n",
    "keyboard_  = [4, 3, 3, 3, 2, 2, 3, 2, 1, 3, 2, 1, 3, 2, 1, 0, 3, 2, 1, 3, 2, 1, 3, 2, 2, 4, 3]\n",
    "keyboard_u = [6, 5, 6, 4, 5, 5, 3, 4, 4, 2, 3, 3, 1, 2, 2, 3, 0, 1, 2, 1, 1, 2, 2, 2, 2, 3, 3]\n",
    "keyboard_h = [6, 5, 5, 4, 4, 4, 3, 3, 3, 2, 2, 2, 1, 1, 1, 2, 1, 0, 1, 2, 1, 1, 3, 2, 2, 4, 3]\n",
    "keyboard_b = [6, 5, 5, 4, 4, 4, 3, 3, 3, 2, 2, 2, 2, 1, 1, 1, 2, 1, 0, 2, 1, 1, 3, 2, 2, 4, 3]\n",
    "keyboard_i = [7, 6, 7, 5, 6, 6, 4, 5, 5, 3, 4, 4, 2, 3, 3, 3, 1, 2, 2, 0, 1, 2, 1, 1, 2, 2, 2]\n",
    "keyboard_j = [7, 6, 6, 5, 5, 5, 4, 4, 4, 3, 3, 3, 2, 2, 2, 2, 1, 1, 1, 1, 0, 1, 2, 1, 1, 3, 2]\n",
    "keyboard_n = [7, 6, 6, 5, 5, 5, 4, 4, 4, 3, 3, 3, 2, 2, 2, 1, 2, 1, 1, 2, 1, 0, 2, 1, 1, 3, 2]\n",
    "keyboard_o = [8, 7, 8, 6, 7, 7, 5, 6, 6, 4, 5, 5, 3, 4, 4, 3, 2, 3, 3, 1, 2, 2, 0, 1, 2, 1, 1]\n",
    "keyboard_k = [8, 7, 7, 6, 6, 6, 5, 5, 5, 4, 4, 4, 3, 3, 3, 2, 2, 2, 2, 1, 1, 1, 1, 0, 1, 2, 1]\n",
    "keyboard_m = [8, 7, 7, 6, 6, 6, 5, 5, 5, 4, 4, 4, 3, 3, 3, 2, 2, 2, 2, 2, 1, 1, 2, 1, 0, 2, 1]\n",
    "keyboard_p = [9, 8, 9, 7, 8, 8, 6, 7, 7, 5, 6, 6, 4, 5, 5, 4, 3, 4, 4, 2, 3, 3, 1, 2, 2, 0, 1]\n",
    "keyboard_l = [9, 8, 8, 7, 7, 7, 6, 6, 6, 5, 5, 5, 4, 4, 4, 3, 3, 3, 3, 2, 2, 2, 1, 1, 1, 1, 0]\n",
    "\n",
    "\n",
    "keyboard = [keyboard_q, keyboard_w, keyboard_a, keyboard_e, keyboard_s, keyboard_z, keyboard_r, keyboard_d, keyboard_x,\n",
    "            keyboard_t, keyboard_f, keyboard_c, keyboard_y, keyboard_g, keyboard_v, keyboard_, keyboard_u, keyboard_h,\n",
    "            keyboard_b, keyboard_i, keyboard_j, keyboard_n, keyboard_o, keyboard_k, keyboard_m, keyboard_p, keyboard_l]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "# create new matrix with distance\n",
    "\n",
    "def matrix_distance(m_distance):\n",
    "    actual = 0\n",
    "    row_conf = 0\n",
    "    new_matrix = []\n",
    "    matrix_to_return = []\n",
    "    indexes = []\n",
    "    summ = []\n",
    "\n",
    "    for name, c_matrix in m_distance:\n",
    "        for keyb in keyboard:\n",
    "            for i in range(0, max(keyb)+1):\n",
    "                for j in keyb:\n",
    "                    if i == j:\n",
    "                        indexes.append(actual)\n",
    "                    actual += 1\n",
    "\n",
    "                # print(indexes)\n",
    "                sum_values = sum(c_matrix[row_conf][indexes])\n",
    "                summ.append(sum_values)\n",
    "                indexes = []\n",
    "                actual = 0\n",
    "            row_conf += 1\n",
    "\n",
    "            new_matrix.append(summ)\n",
    "            summ = []\n",
    "\n",
    "        matrix_to_return.append((name, new_matrix))\n",
    "        new_matrix = []\n",
    "        row_conf = 0\n",
    "\n",
    "    return matrix_to_return\n",
    "\n",
    "\n",
    "# print(matrix_distance(copy_conf))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# create plot of matrix with distance\n",
    "\n",
    "def create_plot_distance(copy_matrix, path_file):\n",
    "\n",
    "    for name_alg, matrix in matrix_distance(copy_matrix):\n",
    "\n",
    "        conf_matrix_distance = pd.DataFrame(matrix, index = idx, columns = range(0, 10))\n",
    "\n",
    "        plt.figure(figsize = (24,14), dpi=100)\n",
    "        sns.set(font_scale=1)\n",
    "\n",
    "        sns.heatmap(conf_matrix_distance, annot=True, yticklabels=zip_labels, xticklabels=range(0, 10), annot_kws={\"fontsize\":10}, fmt='.0f')\n",
    "\n",
    "        plt.title(name_alg + \"_distance\", fontweight='bold')\n",
    "        plt.ylabel(\"actual\", fontweight='bold')\n",
    "        plt.xlabel(\"distance\", fontweight='bold')\n",
    "\n",
    "        plt.savefig(\"C:\\\\Users\\\\giuli\\\\Desktop\\\\image finale\\\\confusion_matrix\\\\\" + path_file + \"\\\\\" + name_alg +\n",
    "                        \"_distance.png\", bbox_inches='tight')\n",
    "\n",
    "        plt.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "create_plot_distance(copy_conf, \"distanze non scalate\")\n",
    "create_plot_distance(copy_conf_scaled, \"distanze scalate\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier\n",
      "GaussianNB\n",
      "SVC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\giuli\\desktop\\pythontesi\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\giuli\\desktop\\pythontesi\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\giuli\\desktop\\pythontesi\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier\n",
      "AdaBoostClassifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\giuli\\desktop\\pythontesi\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\giuli\\desktop\\pythontesi\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\giuli\\desktop\\pythontesi\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\giuli\\desktop\\pythontesi\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\giuli\\desktop\\pythontesi\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\giuli\\desktop\\pythontesi\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier\n"
     ]
    }
   ],
   "source": [
    "# plot F1-SCORE, PRECISION and RECALL\n",
    "\n",
    "alphabet = [' ', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p',\n",
    "            'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "\n",
    "# set width of bars\n",
    "bar_width = 0.25\n",
    "\n",
    "for name, prediction in predictions:\n",
    "    print(name)\n",
    "    report = classification_report(y_test, prediction, output_dict=True)\n",
    "    data_report = pd.DataFrame(report).transpose()\n",
    "    precision = round(data_report.loc[:, 'precision'], 2)\n",
    "    recall = round(data_report.loc[:, 'recall'], 2)\n",
    "    f_score = round(data_report.loc[:, 'f1-score'], 2)\n",
    "    prec = precision.head(27)\n",
    "    rec = recall.head(27)\n",
    "    f = f_score.head(27)\n",
    "\n",
    "    # print(name, f1_score(y_test, prediction, average=None))\n",
    "\n",
    "    # Set position of bar on X axis\n",
    "    r1 = np.arange(27)\n",
    "    r2 = [xindex + bar_width for xindex in r1]\n",
    "    r3 = [xindex + bar_width for xindex in r2]\n",
    "\n",
    "    # Make the plot\n",
    "    plt.figure(figsize = (24,14), dpi=100)\n",
    "    plt.bar(r1, prec, width=bar_width, edgecolor='white', label='precision')\n",
    "    plt.bar(r2, rec, width=bar_width, edgecolor='white', label='recall')\n",
    "    plt.bar(r3, f, width=bar_width, edgecolor='white', label='f1-score')\n",
    "\n",
    "    # Add xticks and yticks on the middle of the group bars\n",
    "    # plt.xlabel('labels', fontweight='bold')\n",
    "    plt.title(\"Precision, Recall, F1 Score: \" + name, fontweight='bold')\n",
    "    plt.yticks(np.arange(0, 1.05, 0.05), fontsize=15)\n",
    "    plt.xticks([r + bar_width for r in range(27)], alphabet, fontsize=20)\n",
    "\n",
    "    # create legend, save graphic\n",
    "    plt.legend(prop={\"size\":20})\n",
    "\n",
    "    plt.savefig(\"C:\\\\Users\\\\giuli\\\\Desktop\\\\image finale\\\\f_score\\\\\" + name + \"_f_score.png\",\n",
    "                bbox_inches='tight')\n",
    "    plt.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}